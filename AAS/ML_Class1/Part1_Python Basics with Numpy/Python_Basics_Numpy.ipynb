{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Python Basics with Numpy \n",
    "\n",
    "This assignment gives you a brief introduction on how to code some useful functions in Python 3. \n",
    "\n",
    "**After this assignment you will:**\n",
    "- Be able to use iPython Notebooks (create Python files with extention .IPYNB)\n",
    "- Be able to use numpy functions and numpy matrix/vector operations such as np.sum, np.dot, np.multiply, np.maximum,  np.exp, np.log, and np.reshape.\n",
    "- Understand the concept of \"broadcasting\"\n",
    "- Be able to vectorize code and compute L1 and L2 loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "Project Jupyter proposed in 2014 by Fernando Pérez. \n",
    "Reference to the 3 core programming languages supported Julia, Python, R. \n",
    "Open-source software, open-standards,  language agnostic \n",
    "web-based interactive computational environment for creating notebook documents. \n",
    "Ordered list of cells which can contain code, text (Markdown: using Markup language – html, latex), maths, plots. \n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this course. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run\" in the upper bar of the notebook. \n",
    "\n",
    "To undo text entry press \"CTRL+Z\". \n",
    "\n",
    "**Exercise**: Set test to `\"Hello World\"` in the cell below to print \"Hello World\" and run the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Hello World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"test: \" + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "test: Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Building basic functions with numpy ##\n",
    "\n",
    "Numpy is the main package for scientific computing in Python. It is maintained by a large community (www.numpy.org). In this exercise you will learn several numpy functions such as np.exp, np.log, np.reshape.\n",
    "\n",
    "### 1.1 - sigmoid function, np.exp() ###\n",
    "\n",
    "Before using np.exp(), you will use math.exp() to implement the sigmoid function. You will then see why np.exp() is preferable to math.exp().\n",
    "\n",
    "**Exercise**: Build a function that returns the sigmoid of a real number x. Use **math.exp(x)** for the exponential function.\n",
    "\n",
    "**Note**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is also known as the logistic function. It is a non-linear function used both in Machine Learning (e.g. Logistic Regression method) and Deep Learning (DL).\n",
    "\n",
    "<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n",
    "\n",
    "To refer to a function belonging to a specific package you could call it using package_name.function(). Run the code below to see an example with math.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- a scalar value\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "   \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<table style = \"width:40%\">\n",
    "    <tr>\n",
    "    <td>** basic_sigmoid(3) **</td> \n",
    "        <td>0.9525741268224334 </td> \n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs of the functions in \"math\" library are scalar real numbers. \n",
    "\n",
    "In Machine Learning (ML) we mostly use matrices and vectors. This is why numpy is more useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One reason why we use \"numpy\" instead of \"math\" library\n",
    "x = [1, 2, 3] #lista \n",
    "basic_sigmoid(x) # you will see this gives an error when you run it, because x is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $ x = (x_1, x_2, ..., x_n)$ is a row vector then $np.exp(x)$ will apply the exponential function to every element of x. \n",
    "\n",
    "The output will be also a vector: $np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # this means you can access numpy functions by writing np.function_name() instead of numpy.function_name()\n",
    "\n",
    "# example of np.exp\n",
    "#Use np.array() do create the array [1, 2, 3].\n",
    "x=np.array([1, 2, 3])\n",
    "\n",
    "# Apply np.exp() to this array and print the result. Run the cell\n",
    "print(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If x is a vector, then a Python operation such as $s = x + 3$ or $s = \\frac{1}{x}$ will output s as a vector of the same size as x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of vector operation\n",
    "\n",
    "#Use np.array() do create the array [1, 2, 3].\n",
    "x=np.array([1, 2, 3])\n",
    "\n",
    "#Print the result of operation (x+3)\n",
    "\n",
    "print(x+3)\n",
    "#Print the result of operation (1/x)\n",
    "print(1/x)\n",
    "#Run the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need more info on a numpy function, look at [the official documentation](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html). \n",
    "\n",
    "You can also create a new cell in the notebook and write `np.exp?` (for example) to get quick access to the documentation.\n",
    "\n",
    "**Exercise**: Implement the sigmoid function using numpy. \n",
    "\n",
    "**Instructions**: x can now be a real number, vector, or matrix. \n",
    "\n",
    "Data structures we use in numpy to represent these shapes (vectors, matrices) are called numpy arrays. \n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments: x - a scalar or numpy array of any size\n",
    "\n",
    "    Return:  s - sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = ?\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:      \n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid([1,2,3])**</td> \n",
    "        <td> array([ 0.73105858,  0.88079708,  0.95257413]) </td> \n",
    "    </tr>\n",
    "</table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Sigmoid gradient\n",
    "\n",
    "Gradients are very important to train ML models.\n",
    "\n",
    "**Exercise**: Implement function *sigmoid_derivative()* to compute the gradient of the sigmoid function with respect to its input x. \n",
    "The formula is: $sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$\n",
    "\n",
    "You can choose to code this function in two steps:\n",
    "1. Set s to be the sigmoid of x. \n",
    "2. Compute $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variable s and then use it to compute the gradient.\n",
    "    \n",
    "    Arguments:  x - a scalar number or numpy array\n",
    "\n",
    "    Return:    ds - the computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    s = ?\n",
    "    \n",
    "    ds = ?\n",
    "        \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid_derivative([1,2,3])**</td> \n",
    "        <td> [ 0.19661193  0.10499359  0.04517666] </td> \n",
    "    </tr>\n",
    "</table> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Reshaping arrays ###\n",
    "\n",
    "Two useful numpy functions are [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) and [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape is used to get the shape (dimension) of a matrix/vector X. \n",
    "- X.reshape(...) is used to reshape X into some other dimension. \n",
    "\n",
    "For example, in computer science, an image is represented by a 3D array of shape $(length, height, depth = 3)$. However, when you read an image as the input of an algorithm you may need to convert it to a vector of shape $(length*height*3, 1)$. In other words, you \"unroll\", or reshape, the 3D array into a 1D vector.\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**Exercise**:`image2vector()` takes an input of shape (length, height, 3) and returns a vector of shape (length\\*height\\*3, 1). \n",
    "\n",
    "For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:\n",
    "``` python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) \n",
    "# v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```\n",
    "- Extract the image dimensions with `image.shape[?]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument: image, a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:    v, a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    length = image.shape[0]\n",
    "    height = image.shape[1]\n",
    "    depth = image.shape[2]\n",
    "    v = image.reshape(length*height*depth, 1)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RGB image is typically  (Npx_x, Npx_y,3) where 3 represents the RGB values\n",
    "# generate 3x2x3 data volume\n",
    "image = np.array([[[ 0.67826139,  0.29380381],  \n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "#check the shape of image\n",
    "print(image.shape)\n",
    "\n",
    "#Call image2vector to reshape the image \n",
    " ?\n",
    "# check the new shape \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <tr> \n",
    "       <td> **image2vector(image)** </td> \n",
    "       <td> [[ 0.67826139]\n",
    " [ 0.29380381]\n",
    " [ 0.90714982]\n",
    " [ 0.52835647]\n",
    " [ 0.4215251 ]\n",
    " [ 0.45017551]\n",
    " [ 0.92814219]\n",
    " [ 0.96677647]\n",
    " [ 0.85304703]\n",
    " [ 0.52351845]\n",
    " [ 0.19981397]\n",
    " [ 0.27417313]\n",
    " [ 0.60659855]\n",
    " [ 0.00533165]\n",
    " [ 0.10820313]\n",
    " [ 0.49978937]\n",
    " [ 0.34144279]\n",
    " [ 0.94630077]]</td> \n",
    "     </tr>\n",
    "    \n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Normalizing rows and Broadcasting\n",
    "\n",
    "Another useful technique in ML is to normalize the data. The  optimization algorithms converge faster after normalization. There are different types of normalization, here we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm 2).\n",
    "\n",
    "For example, if $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$\n",
    "\n",
    " norm 2 is computed as: $\\sqrt(0^2+3^2+4^2) =5$ (norm of row 1) and $\\sqrt(2^2+6^2+4^2) =\\sqrt(56)$ (norm of row 2)   \n",
    " \n",
    " and\n",
    "\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ \n",
    "\n",
    "The norm of each row of $x\\_normalized $ is 1, known also as unit length.  \n",
    "\n",
    "**Exercise**: Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).\n",
    "\n",
    "\n",
    "Note that you can divide matrices of different sizes: x.shape=(2,3) ,  x_norm.shape=(2,1), and **x/x_norm** works fine due to **python broadcasting**.\n",
    "\n",
    "Python will copy x_norm 3 times and will apply for each column of x. \n",
    "\n",
    "An important concept in numpy is \"**broadcasting**\". It is very useful for performing math operations between arrays of different shapes. \n",
    "\n",
    "For more inf on broadcasting, read the official [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
    "\n",
    "**General Principle of Broadcasting**: \n",
    "\n",
    "A is (m,n) matrix, B is (1,n) matrix  => Python will copy m times B and will do element-wise operations (+, -, *, /) between A and B. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You may choose a different name.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., axis = ..., keepdims = True)\n",
    "    #In Python axis=0 is the vertical axis (columns) &  axis=1 is the horizontal axis (rows)\n",
    "    x_norm = np.linalg.norm(x,axis=1,keepdims=True)\n",
    "    \n",
    "   \n",
    "    # Divide x by its norm to get the normalized (by row) matrix\n",
    "    x = ?\n",
    "   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [2, 6, 4]])\n",
    "\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:60%\">\n",
    "\n",
    "     <tr> \n",
    "       <td> **normalizeRows(x)** </td> \n",
    "       <td> [[ 0.          0.6         0.8       ]\n",
    " [ 0.13736056  0.82416338  0.54944226]]</td> \n",
    "     </tr>\n",
    "    \n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2) Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHENEVER POSSIBLE AVOID EXPLICIT FOR-LOOPs !!!**\n",
    "\n",
    "In ML, we deal with large datasets. Hence, a non-computationally-optimal function can result in a model that takes ages to run. \n",
    "\n",
    "The vectorized implementation is more efficient. \n",
    "\n",
    "**Note** that `np.dot()` performs a matrix-matrix or matrix-vector multiplication. This is different from `np.multiply()` and`*` operator which perform an element-wise multiplication ( equivalent to  `.*` in Matlab/Octave). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to tell the difference between the following implementations of dot product.\n",
    "\n",
    "import time\n",
    "a=np.random.rand(1000000) \n",
    "#print(a.shape)  #  (1000000,)\n",
    "b=np.random.rand(1000000)\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic=time.time()\n",
    "c=np.dot(a,b)\n",
    "toc=time.time()\n",
    "print ( \"vectorized version= \" + str(1000*(toc-tic)) + \"ms\")\n",
    "print(c)\n",
    "\n",
    "### FOR LOOP DOT PRODUCT OF VECTORS ###\n",
    "c=0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc=time.time()\n",
    "print ( \" for loop = \" + str(1000*(toc-tic)) + \"ms\")\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement L1 and L2 loss functions\n",
    "\n",
    "**Exercise**: Implement the numpy vectorized version to compute L1 loss function. abs(x) (absolute value of x) is useful function. \n",
    "\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted values/labels)\n",
    "    y -- vector of size m (true value/labels)\n",
    "    \n",
    "    Returns:loss - the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = ?\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "\n",
    "     <tr> \n",
    "       <td> **L1** </td> \n",
    "       <td> 1.1 </td> \n",
    "     </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the numpy vectorized version to compute L2 loss function: $\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted value/labels)\n",
    "    y -- vector of size m (true value/labels)\n",
    "    \n",
    "    Returns: loss - the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "     loss = ?\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x**2, np.power(x, 2), np.square?\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x**2, np.power(x, 2), np.square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **L2** </td> \n",
    "       <td> 0.43 </td> \n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on python/numpy vectors\n",
    "\n",
    "Broadcasting operations have both strengths and weaknesses. Strength because it lets you get a lot done even with just a single line of code. But there's also weakness because with broadcasting sometimes it's possible you can introduce very subtle strange looking bugs. For example, if you take a column vector and add it to a row vector, you would expect an error message. But you might actually get back a matrix as a sum of a row vector and a column vector. \n",
    "\n",
    "Don't use data structures where the shape is (n, ), i.e. rank 1 array. \n",
    "Instead, every time you create an array, make it a column or a row vector.  \n",
    "\n",
    "Check this with the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(5)  # rank 1 array \n",
    "print(a.shape)\n",
    "\n",
    "#Note that there is no difference between a and a_transposed (a.T)\n",
    "#print(a)\n",
    "#print(a.T)\n",
    "\n",
    "#You can reshape rank 1 array to have two dimensions\n",
    "#a=a.reshape((5,1))\n",
    "#print(a.shape)\n",
    "\n",
    "#or better use \n",
    "#b = np.random.randn(5,1) \n",
    "#print(b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
