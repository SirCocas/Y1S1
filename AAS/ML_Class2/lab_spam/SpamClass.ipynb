{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAS lab  -  Spam Classification with SVM\n",
    "\n",
    "Objectives: Apply Support Vector Machine (SVM) to build a spam classifier. \n",
    "\n",
    "You will train a SVM classifier to classify whether a given email is spam (y = 1) or non-spam (y = 0). \n",
    "\n",
    "First, each email needs to be converted into a feature vector x. The dataset used in this exercise is based on a subset of SpamAssassin Public Corpus (https://spamassassin.apache.org/old/publiccorpus/ ) You will only use the body of the email (excluding the email headers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import numpy as np\n",
    "# pandas - Python Data Analysis Library\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#to load matlab mat files\n",
    "from scipy.io import loadmat\n",
    "#Re package is used for text processing. \n",
    "import re\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one sample\n",
    "file_contents = open(\"emailSample1.txt\",\"r\").read()\n",
    "\n",
    "print(file_contents )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing emails. Construct a Vocabulary List\n",
    "<img src=\"images/f8.png\" style=\"width:500px;height:150px;\">\n",
    "<caption><center> **Fig.1 ** : **Sample email** </center></caption>\n",
    "\n",
    "Fig.1 shows a sample email that contains a URL, an email address (at the end), numbers, and dollar amounts. While many emails contain similar types of entities (e.g., numbers, URLs, email addresses), the specific entities (e.g., the specific URL or specific dollar amount) will be different in almost every email. Therefore, one method often employed in processing emails is to “normalize\" these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, we will replace each URL in the email with the unique string “httpaddr\" to indicate that a URL was present. This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small. \n",
    "\n",
    "The words are also transformated into lower case letters and are reduced to their stemmed form. For example, “discount\", “discounts\", “discounted\" and “discounting\" are all replaced with “discount\". Sometimes, the Stemmer actually strips off additional characters from the end, so “include\", “includes\", ”included\", and “including\" are all replaced with “includ\".\n",
    "\n",
    "After the preprocessing we get a list of words for each email, as shown in Fig.2 for this example. \n",
    "\n",
    "<img src=\"images/f7.png\" style=\"width:450px;height:150px;\">\n",
    "<caption><center> **Fig.2 ** : **Preprocessed sample email** </center></caption>\n",
    "\n",
    "The next step is to choose which words to use in the classiﬁer and which to leave out. The vocabulary list (file *vocab.txt*) was selected by choosing all words which occur at least a 100 times in the training set of emails (corpus), resulting in a list of 1899 words (Fig.3). Words that occur rarely in the training set were excluded, because they may cause the model to overﬁt the training set. In practice, a vocabulary list with about 10,000 to 50,000 words is often used. \n",
    "\n",
    "<img src=\"images/f5.png\" style=\"width:100px;height:150px;\">\n",
    "<caption><center> **Fig.3 ** : **Vocabulary list** </center></caption>\n",
    "\n",
    "Given the vocabulary list, we can now map each word in the preprocessed emails (Fig.2) into a list of word indices that contains the index of the word in the vocabulary list. Fig.4 shows the mapping for the sample email. Speciﬁcally, in the sample email, the word “anyone” was ﬁrst normalized to “anyon” and then mapped onto the index 86 in the vocabulary list.  \n",
    "\n",
    "<img src=\"images/f6.png\" style=\"width:150px;height:200px;\">\n",
    "<caption><center> **Fig.4 ** : **Word indices for sample email** </center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and transform vacablist into a dictionary \n",
    "vocabList = open(\"vocab.txt\",\"r\").read()\n",
    "\n",
    "#print(vocabList)\n",
    "#print(len(vocabList)) #=20240, counts all letters, spaces\n",
    "\n",
    "vocabList=vocabList.split(\"\\n\")[:-1]\n",
    "\n",
    "vocabList_d={}\n",
    "for ea in vocabList:\n",
    "    #key is the word; value is the index\n",
    "    value,key = ea.split(\"\\t\")[:]\n",
    "    vocabList_d[key] = value\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function *processEmail* performs mapping between words and indices. It gets an email and transforms it into a list of words. For each word it looks up if the word exist in the vocabulary list. If the word exists, it adds the index of the word into the word indices variable. If the word does not exist, and is therefore not in the vocabulary, it skips the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processEmail(email_contents,vocabList_d):\n",
    "    \"\"\"\n",
    "    Preprocesses the body of an email and returns a list of indices of the words contained \n",
    "    in the email. \n",
    "    \"\"\"\n",
    "    # Lower case\n",
    "    email_contents = email_contents.lower()\n",
    "    \n",
    "    # Handle numbers\n",
    "    email_contents = re.sub(\"[0-9]+\",\"number\",email_contents)\n",
    "    \n",
    "    # Handle URLS\n",
    "    email_contents = re.sub(\"[http|https]://[^\\s]*\",\"httpaddr\",email_contents)\n",
    "    \n",
    "    # Handle Email Addresses\n",
    "    email_contents = re.sub(\"[^\\s]+@[^\\s]+\",\"emailaddr\",email_contents)\n",
    "    \n",
    "    # Handle $ sign\n",
    "    email_contents = re.sub(\"[$]+\",\"dollar\",email_contents)\n",
    "    \n",
    "    # Strip all special characters\n",
    "    specialChar = [\"<\",\"[\",\"^\",\">\",\"+\",\"?\",\"!\",\"'\",\".\",\",\",\":\"]\n",
    "    for char in specialChar:\n",
    "        email_contents = email_contents.replace(str(char),\"\")\n",
    "    email_contents = email_contents.replace(\"\\n\",\" \")    \n",
    "    \n",
    "    # Stem the word\n",
    "    ps = PorterStemmer()\n",
    "    email_contents = [ps.stem(token) for token in email_contents.split(\" \")]\n",
    "    email_contents= \" \".join(email_contents)\n",
    "    \n",
    "    # Process the email and return word_indices\n",
    "    \n",
    "    word_indices=[]\n",
    "    \n",
    "    for char in email_contents.split():\n",
    "        if len(char) >1 and char in vocabList_d:\n",
    "            word_indices.append(int(vocabList_d[char]))\n",
    "    \n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_indices= processEmail(file_contents,vocabList_d)\n",
    "print(word_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting binary features from emails\n",
    "\n",
    "You will now implement the feature extraction that converts each email into a vector. The binary feature $x_i$ for an email corresponds to whether the i-th word in the dictionary occurs in the email. That is, $x_i$ = 1 if the i-th word is in the email and $x_i$ = 0 if the i-th word is not present in the email. Thus, for a typical email, this feature would look like, n is the number of words in the vocabulary list: \n",
    "\n",
    "<img src=\"images/f9.png\" style=\"width:100px;height:180px;\">\n",
    "<caption><center> **Fig.5 ** : **Binary feature vector** </center></caption>\n",
    "\n",
    "You should now complete function *emailFeatures* to generate a feature vector for an email, given the word indices. Once you run *emailFeatures* on the email sample, you should see that the feature vector has length 1899 and 43 non-zero entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emailFeatures(word_indices, vocabList_d):\n",
    "    \"\"\"\n",
    "    Takes in a word_indices vector and  produces a feature vector from the word indices. \n",
    "    \"\"\"\n",
    "    n = len(vocabList_d)\n",
    "    \n",
    "    features = np.zeros((n,1))\n",
    "    \n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ?\n",
    "print(\"Length of feature vector: \",?)  #ANSWER: 1899\n",
    "print(\"Number of non-zero entries: \", ?  #ANSWER: 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SVM for spam classification\n",
    "\n",
    "The next step will load a preprocessed training dataset that will be used to train a SVM classiﬁer. \n",
    "\n",
    "*spamTrain.mat* contains 4000 training examples and their labels of spam/non-spam emails.\n",
    "\n",
    "*spamTest.mat* contains 1000 test examples. \n",
    "\n",
    "Each original email was processed using the *processEmail* and *emailFeatures* functions and converted into a vector $x^{(i)}$ ∈ $R^{1899}$. After loading the train dataset, the main script will proceed to train a SVM to classify between spam (y = 1) and non-spam (y = 0) emails. Once the training completes, you should see that the classiﬁer gets a training accuracy of about 99.8% and a test accuracy of about 98.9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use loadmat to load the file spamTrain.mat as a dictionary with keys \"X\"  and \"y\" \n",
    "\n",
    "spam_mat = loadmat(\"spamTrain.mat\")\n",
    "print(spam_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the training data from the keys \"X\"  and \"y\" \n",
    "\n",
    "X_train = ?\n",
    "y_train = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Support Vector Classifier (SVC) to train (fit) binary classifier and compute the training accuracy. \n",
    "#Suggestion: Call SVC with linear kernel and C=0.1\n",
    "from sklearn.svm import SVC \n",
    "SVC?\n",
    "print(\"Training Accuracy: ?\")  # Answer:~ 99.8 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use loadmat to load the file spamTest.mat as a dictionary with keys \"Xtest\"  and \"ytest\" \n",
    "#and extract the testing data\n",
    "\n",
    "X_test = ?\n",
    "y_test = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the trained SVC classifier to predict the test data and compute Test accuracy\n",
    "\n",
    "print(\"Test Accuracy:?\"  # ANSWER: 98.9 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top predictors for spam\n",
    "\n",
    "To better understand how the spam classiﬁer works, we can inspect the parameters to see which words the classiﬁer thinks are the most predictive of spam. The next step ﬁnds the parameters with the largest positive values in the classiﬁer and displays the corresponding words. Thus, if an email contains words such as \"click\", “guarantee”, “remove”, “dollar”, “price”, etc., it is likely to be classiﬁed as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = spam_svc.coef_[0]  # print(weights.shape)  (1899,)\n",
    "\n",
    "#first column indices (1,2,3,... 1899), second column all weights \n",
    "weights_col = np.hstack((np.arange(1,1900).reshape(1899,1),weights.reshape(1899,1)))\n",
    "\n",
    "#transform it into data frame \n",
    "df = pd.DataFrame(weights_col)\n",
    "\n",
    "df.sort_values(by=[1],ascending = False,inplace=True)\n",
    "\n",
    "predictors = []\n",
    "idx=[]\n",
    "for i in df[0][:15]:\n",
    "    for keys, values in vocabList_d.items():\n",
    "        if str(int(i)) == values:\n",
    "            predictors.append(keys)\n",
    "            idx.append(int(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top predictors of spam:\")\n",
    "\n",
    "for _ in range(15):\n",
    "    print(predictors[_],\"\\t\\t\",round(df[1][idx[_]-1],6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have included two email examples (*emailSample1.txt*, *emailSample2.txt*) and two spam examples (*spamSample1.txt*, *spamSample2.txt*) as test emails. Apply the learned SVM spam classifier to see if the classiﬁer gets them right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = open(\"spamSample1.txt\",\"r\").read()\n",
    "\n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try your own emails\n",
    "\n",
    "\n",
    "Try your own emails by replacing the examples (plain text ﬁles) with your own emails.\n",
    "Now that you have trained a spam classiﬁer, you can try it out on your own emails.\n",
    "\n",
    "\n",
    "### Build your own dataset\n",
    "\n",
    "In this project, we provided a preprocessed training set and test set. These datasets were created using the same functions (processEmail and emailFeatures) that you have completed. Download the original ﬁles from the public corpus, run the *processEmail* and *emailFeatures* functions on each email to extract a feature vector from each email. This will allow you to build a dataset X, y of examples. You should then randomly divide the dataset into training, cross validation and test sets.\n",
    "\n",
    "While you are building your own dataset, you may also build your own vocabulary list (by selecting the high frequency words that occur in the dataset) and adding any additional features that you find useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
