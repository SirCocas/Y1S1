@article{31f29890930d7bd4c1adb457adc9540f3e176a98,
title = {Experimenting with error abstraction in requirements documents},
year = {1998},
url = {https://www.semanticscholar.org/paper/31f29890930d7bd4c1adb457adc9540f3e176a98},
abstract = {In previous experiments we showed that the Perspective-Based Reading (PBR) family of defect detection techniques was effective at detecting faults in requirements documents in some contexts. Experiences from these studies indicate that requirements faults are very difficult to define, classify and quantify. In order to address these difficulties, we present an empirical study whose main purpose is to investigate whether defect detection in requirements documents can be improved by focusing on the errors (i.e., underlying human misconceptions) in a document rather than the individual faults that they cause. In the context of a controlled experiment, we assess both benefits and costs of the process of abstracting errors from faults in requirements documents.},
author = {F. Lanubile and F. Shull and V. Basili},
journal = {Proceedings Fifth International Software Metrics Symposium. Metrics (Cat. No.98TB100262)},
volume = {},
pages = {114-121},
doi = {10.1109/METRIC.1998.731236},
}

@article{b924cf12e9a7d31b2b69f85394892e3b58a0f7d9,
title = {Property-based attestation for computing platforms: caring about properties, not mechanisms},
year = {2004},
url = {https://www.semanticscholar.org/paper/b924cf12e9a7d31b2b69f85394892e3b58a0f7d9},
abstract = {Over the past years, the computing industry has started various initiatives announced to increase computer security by means of new hardware architectures. The most notable effort is the Trusted Computing Group (TCG) and the Next-Generation Secure Computing Base (NGSCB). This technology offers useful new functionalities as the possibility to verify the integrity of a platform (attestation) or binding quantities on a specific platform (sealing).In this paper, we point out the deficiencies of the attestation and sealing functionalities proposed by the existing specification of the TCG: we show that these mechanisms can be misused to discriminate certain platforms, i.e., their operating systems and consequently the corresponding vendors. A particular problem in this context is that of managing the multitude of possible configurations. Moreover, we highlight other shortcomings related to the attestation, namely system updates and backup. Clearly, the consequences caused by these problems lead to an unsatisfactory situation both for the private and business branch, and to an unbalanced market when such platforms are in wide use.To overcome these problems generally, we propose a completely new approach: the attestation of a platform should not depend on the specific software or/and hardware (configuration) as it is today's practice but only on the "properties" that the platform offers. Thus, a property-based attestation should only verify whether these properties are sufficient to fulfill certain (security) requirements of the party who asks for attestation. We propose and discuss a variety of solutions based on the existing Trusted Computing (TC) functionality. We also demonstrate, how a property-based attestation protocol can be realized based on the existing TC hardware such as a Trusted Platform Module (TPM).},
author = {A. Sadeghi and Christian Stüble},
doi = {10.1145/1065907.1066038},
}

@article{84eae27f026895f5a8acf61ad3299c6d9ef44026,
title = {Using error abstraction and classification to improve requirement quality: conclusions from a family of four empirical studies},
year = {2013},
url = {https://www.semanticscholar.org/paper/84eae27f026895f5a8acf61ad3299c6d9ef44026},
abstract = {Achieving high software quality is a primary concern for software development organizations. Researchers have developed many quality improvement methods that help developers detect faults early in the lifecycle. To address some of the limitations of fault-based quality improvement approaches, this paper describes an approach based on errors (i.e. the sources of the faults). This research extends Lanubile et al.’s, error abstraction process by providing a formal requirement error taxonomy to help developers identify both faults and errors. The taxonomy was derived from the software engineering and psychology literature. The error abstraction and classification process and the requirement error taxonomy are validated using a family of four empirical studies. The main conclusions derived from the four studies are: (1) the error abstraction and classification process is an effective approach for identifying faults; (2) the requirement error taxonomy is useful addition to the error abstraction process; and (3) deriving requirement errors from cognitive psychology research is useful.},
author = {G. Walia and Jeffrey C. Carver},
journal = {Empirical Software Engineering},
volume = {18},
pages = {625-658},
doi = {10.1007/s10664-012-9202-3},
}

@article{38d28435a1ce32c47927b1e0e687f32b5daea399,
title = {Human Error and General Aviation Accidents: A Comprehensive, Fine-Grained Analysis Using HFACS},
year = {2005},
url = {https://www.semanticscholar.org/paper/38d28435a1ce32c47927b1e0e687f32b5daea399},
abstract = {Abstract : The Human Factors Analysis and Classification System (HFACS) is a theoretically based tool for investigating and analyzing human error associated with accidents and incidents. Previous research performed at both the University of Illinois and the Civil Aerospace Medical Institute has successfully shown that HFACS can be reliably used to analyze the underlying human causes of both commercial and general aviation (GA) accidents. These analyses have helped to identify general trends in the types of human factors issues and aircrew errors that have contributed to civil aviation accidents. The next step was to identify the exact nature of the human errors identified. The following questions of interest were addressed: (1) Which unsafe acts are associated with the largest percentage of accidents?; (2) Has the percentage of accidents associated with each unsafe act changed over the years?; (3) Does the pattern of unsafe acts differ across fatal and non-fatal accidents?; (4) Do the patterns of unsafe acts for fatal and non-fatal accidents differ across years?; (5) How often is each error type the "primary" cause of an accident?; (6) Do seminal unsafe acts differ across years or as a function of accident severity (fatal vs. non-fatal)?; and (7) What are the exact types of errors committed within each error category, and do these types of errors differ across accident severity or seminal events? The purpose of this research effort was to address these questions by performing a fine-grained HFACS analysis of the individual human causal factors associated with GA accidents, and to assist in the generation of intervention programs. The report details the findings and offers an approach for developing interventions to address them.},
author = {D. Wiegmann and Troy P. Faaborg and A. Boquet and C. Detwiler and K. Holcomb and S. Shappell},
}

@article{2588e41e4cd9442387791325d241649ff13af33c,
title = {Terra: a virtual machine-based platform for trusted computing},
year = {2003},
url = {https://www.semanticscholar.org/paper/2588e41e4cd9442387791325d241649ff13af33c},
abstract = {We present a flexible architecture for trusted computing, called Terra, that allows applications with a wide range of security requirements to run simultaneously on commodity hardware. Applications on Terra enjoy the semantics of running on a separate, dedicated, tamper-resistant hardware platform, while retaining the ability to run side-by-side with normal applications on a general-purpose computing platform. Terra achieves this synthesis by use of a trusted virtual machine monitor (TVMM) that partitions a tamper-resistant hardware platform into multiple, isolated virtual machines (VM), providing the appearance of multiple boxes on a single, general-purpose platform. To each VM, the TVMM provides the semantics of either an "open box," i.e. a general-purpose hardware platform like today's PCs and workstations, or a "closed box," an opaque special-purpose platform that protects the privacy and integrity of its contents like today's game consoles and cellular phones. The software stack in each VM can be tailored from the hardware interface up to meet the security requirements of its application(s). The hardware and TVMM can act as a trusted party to allow closed-box VMs to cryptographically identify the software they run, i.e. what is in the box, to remote parties. We explore the strengths and limitations of this architecture by describing our prototype implementation and several applications that we developed for it.},
author = {Tal Garfinkel and Ben Pfaff and Jim Chow and M. Rosenblum and D. Boneh},
doi = {10.1145/945445.945464},
}

@article{d6cce73748d986de5990cd52a139102db4fdcd04,
title = {phpSAFE: A Security Analysis Tool for OOP Web Application Plugins},
year = {2015},
url = {https://www.semanticscholar.org/paper/d6cce73748d986de5990cd52a139102db4fdcd04},
abstract = {There is nowadays an increasing pressure to develop complex Web applications at a fast pace. The vast majority is built using frameworks based on third-party server-side plugins that allow developers to easily add new features. However, as many plugin developers have limited programming skills, there is a spread of security vulnerabilities related to their use. Best practices advise the use of systematic code review for assure security, but free tools do not support OOP, which is how most Web applications are currently developed. To address this problem we propose phpSAFE, a static code analyzer that identifies vulnerabilities in PHP plugins developed using OOP. We evaluate phpSAFE against two well-known tools using 35 plugins for a widely used CMS. Results show that phpSAFE clearly outperforms other tools, and that plugins are being shipped with a considerable number of vulnerabilities, which tends to increase over time.},
author = {P. Nunes and J. Fonseca and M. Vieira},
journal = {2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks},
volume = {},
pages = {299-306},
doi = {10.1109/DSN.2015.16},
}

@article{2f028e8b118d061d0a9eb6bddd38ce564ce7de63,
title = {Simulation of Built-in PHP Features for Precise Static Code Analysis},
year = {2014},
url = {https://www.semanticscholar.org/paper/2f028e8b118d061d0a9eb6bddd38ce564ce7de63},
abstract = {The World Wide Web grew rapidly during the last decades and is used by millions of people every day for online shopping, banking, networking, and other activities. Many of these websites are developed with PHP, the most popular scripting language on the Web. However, PHP code is prone to different types of critical security vulnerabilities that can lead to data leakage, server compromise, or attacks against an application's users. This problem can be addressed by analyzing the source code of the application for security vulnerabilities before the application is deployed on a web server. In this paper, we present a novel approach for the precise static analysis of PHP code to detect security vulnerabilities in web applications. As dismissed by previous work in this area, a comprehensive configuration and simulation of over 900 PHP built-in features allows us to precisely model the highly dynamic PHP language. By performing an intra- and inter-procedural data flow analysis and by creating block and function summaries, we are able to efficiently perform a backward-directed taint analysis for 20 different types of vulnerabilities. Furthermore, string analysis enables us to validate sanitization in a context-sensitive manner. Our method is the first to perform fine-grained analysis of the interaction between different types of sanitization, encoding, sources, sinks, markup contexts, and PHP settings. We implemented a prototype of our approach in a tool called RIPS. Our evaluation shows that RIPS is capable of finding severe vulnerabilities in popular real-world applications: we reported 73 previously unknown vulnerabilities in five well-known PHP applications such as phpBB, osCommerce, and the conference management software HotCRP.},
author = {J. Dahse and Thorsten Holz},
doi = {10.14722/NDSS.2014.23262},
}

@article{b9f96d4aa4c9f914a84327c521637804330c06a9,
title = {Automatic detection and correction of web application vulnerabilities using data mining to predict false positives},
year = {2014},
url = {https://www.semanticscholar.org/paper/b9f96d4aa4c9f914a84327c521637804330c06a9},
abstract = {Web application security is an important problem in today's internet. A major cause of this status is that many programmers do not have adequate knowledge about secure coding, so they leave applications with vulnerabilities. An approach to solve this problem is to use source code static analysis to find these bugs, but these tools are known to report many false positives that make hard the task of correcting the application. This paper explores the use of a hybrid of methods to detect vulnerabilities with less false positives. After an initial step that uses taint analysis to flag candidate vulnerabilities, our approach uses data mining to predict the existence of false positives. This approach reaches a trade-off between two apparently opposite approaches: humans coding the knowledge about vulnerabilities (for taint analysis) versus automatically obtaining that knowledge (with machine learning, for data mining). Given this more precise form of detection, we do automatic code correction by inserting fixes in the source code. The approach was implemented in the WAP tool and an experimental evaluation was performed with a large set of open source PHP applications.},
author = {Ibéria Medeiros and N. Neves and M. Correia},
journal = {Proceedings of the 23rd international conference on World wide web},
volume = {},
pages = {},
doi = {10.1145/2566486.2568024},
}

@article{66fb5d6ebd80dd836e6b31879c6d9e764b36d45f,
title = {Precise alias analysis for static detection of web application vulnerabilities},
year = {2006},
url = {https://www.semanticscholar.org/paper/66fb5d6ebd80dd836e6b31879c6d9e764b36d45f},
abstract = {The number and the importance of web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable web applications by means of static source code analysis. To this end, we present a novel, precise alias analysis targeted at the unique reference semantics commonly found in scripting languages. Moreover, we enhance the quality and quantity of the generated vulnerability reports by employing a novel, iterative two-phase algorithm for fast and precise resolution of file inclusions.We integrated the presented concepts into Pixy~\cite{jovanovic06:pixy_short}, a high-precision static analysis tool aimed at detecting cross-site scripting vulnerabilities in PHP scripts. To demonstrate the effectiveness of our techniques, we analyzed three web applications and discovered 106 vulnerabilities. Both the high analysis speed as well as the low number of generated false positives show that our techniques can be used for conducting effective security audits.},
author = {N. Jovanovic and Christopher Krügel and E. Kirda},
doi = {10.1145/1134744.1134751},
}

@article{50d282bd5e076c19943bb01011da5725c0dc92e9,
title = {A case study in root cause defect analysis},
year = {2000},
url = {https://www.semanticscholar.org/paper/50d282bd5e076c19943bb01011da5725c0dc92e9},
abstract = {There are three interdependent factors that drive our software development processes: interval, quality and cost. As market pressures continue to demand new features ever more rapidly, the challenge is to meet those demands while increasing, or at least not sacrificing, quality. One advantage of defect prevention as an upstream quality improvement practice is the beneficial effect it can have on interval: higher quality early in the process results in fewer defects to be found and repaired in the later parts of the process, thus causing an indirect interval reduction. We report a retrospective root cause defect analysis study of the defect Modification Requests (MRs) discovered while building, testing, and deploying a release of a transmission network element product. We subsequently introduced this analysis methodology into new development projects as an in-process measurement collection requirement for each major defect MR. We present the experimental design of our case study discussing the novel approach we have taken to defect and root cause classification and the mechanisms we have used for randomly selecting the MRs to analyze and collecting the analyses via a Web interface. We then present the results of our analyses of the MRs and describe the defects and root causes that we found, and delineate the countermeasures created to either prevent those defects and their root causes or detect them at the earliest possible point in the development process. We conclude with lessons learned from the case study and resulting ongoing improvement activities.},
author = {M. Leszak and D. Perry and D. Stoll},
journal = {Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium},
volume = {},
pages = {428-437},
doi = {10.1145/337180.337232},
}
