@article{aa7a71165d8f125c1505791ba5440544eb03e2ae,
title = {Maat: Automatically Analyzing VirusTotal for Accurate Labeling and Effective Malware Detection},
year = {2020},
url = {https://www.semanticscholar.org/paper/aa7a71165d8f125c1505791ba5440544eb03e2ae},
abstract = {The malware analysis and detection research community relies on the online platform VirusTotal to label Android apps based on the scan results of around 60 antiviral scanners. Unfortunately, there are no standards on how to best interpret the scan results acquired from VirusTotal, which leads to the utilization of different threshold-based labeling strategies (e.g., if ten or more scanners deem an app malicious, it is considered malicious). While some of the utilized thresholds may be able to accurately approximate the ground truths of apps, the fact that VirusTotal changes the set and versions of the scanners it uses makes such thresholds unsustainable over time. We implemented a method, Maat, that tackles these issues of standardization and sustainability by automatically generating a Machine Learning (ML)-based labeling scheme, which outperforms threshold-based labeling strategies. Using the VirusTotal scan reports of 53K Android apps that span one year, we evaluated the applicability of Maat's ML-based labeling strategies by comparing their performance against threshold-based strategies. We found that such ML-based strategies (a) can accurately and consistently label apps based on their VirusTotal scan reports, and (b) contribute to training ML-based detection methods that are more effective at classifying out-of-sample apps than their threshold-based counterparts.},
author = {Aleieldin Salem and Sebastian Banescu and A. Pretschner},
doi = {10.1145/3465361},
arxivid = {2007.00510},
}

@article{4db986cb592533fee1ecde11ec96db8e77dbb809,
title = {An Analysis of Android Malware Classification Services},
year = {2021},
url = {https://www.semanticscholar.org/paper/4db986cb592533fee1ecde11ec96db8e77dbb809},
abstract = {The increasing number of Android malware forced antivirus (AV) companies to rely on automated classification techniques to determine the family and class of suspicious samples. The research community relies heavily on such labels to carry out prevalence studies of the threat ecosystem and to build datasets that are used to validate and benchmark novel detection and classification methods. In this work, we carry out an extensive study of the Android malware ecosystem by surveying white papers and reports from 6 key players in the industry, as well as 81 papers from 8 top security conferences, to understand how malware datasets are used by both. We, then, explore the limitations associated with the use of available malware classification services, namely VirusTotal (VT) engines, for determining the family of an Android sample. Using a dataset of 2.47 M Android malware samples, we find that the detection coverage of VTâ€™s AVs is generally very low, that the percentage of samples flagged by any 2 AV engines does not go beyond 52%, and that common families between any pair of AV engines is at best 29%. We rely on clustering to determine the extent to which different AV engine pairs agree upon which samples belong to the same family (regardless of the actual family name) and find that there are discrepancies that can introduce noise in automatic label unification schemes. We also observe the usage of generic labels and inconsistencies within the labels of top AV engines, suggesting that their efforts are directed towards accurate detection rather than classification. Our results contribute to a better understanding of the limitations of using Android malware family labels as supplied by common AV engines.},
author = {Mohammed Rashed and Guillermo Suarez-Tangil},
journal = {Sensors (Basel, Switzerland)},
volume = {21},
pages = {},
doi = {10.3390/s21165671},
pmid = {34451112},
}

@article{1e68c2843048e24e463928e94f67cbf3c8fd6503,
title = {Towards Accurate Labeling of Android Apps for Reliable Malware Detection},
year = {2020},
url = {https://www.semanticscholar.org/paper/1e68c2843048e24e463928e94f67cbf3c8fd6503},
abstract = {In training their newly-developed malware detection methods, researchers rely on threshold-based labeling strategies that interpret the scan reports provided by online platforms, such as VirusTotal. The dynamicity of this platform renders those labeling strategies unsustainable over prolonged periods, which leads to inaccurate labels. Using inaccurately labeled apps to train and evaluate malware detection methods significantly undermines the reliability of their results, leading to either dismissing otherwise promising detection approaches or adopting intrinsically inadequate ones. The infeasibility of generating accurate labels via manual analysis and the lack of reliable alternatives force researchers to utilize VirusTotal to label apps. In the paper, we tackle this issue in two manners. Firstly, we reveal the aspects of VirusTotalss dynamicity and how they impact threshold-based labeling strategies and provide actionable insights on how to use these labeling strategies given VirusTotal's dynamicity reliably. Secondly, we motivate the implementation of alternative platforms by (a) identifying VirusTotal limitations that such platforms should avoid, and (b) proposing an architecture of how such platforms can be constructed to mitigate VirusTotal's limitations.},
author = {Aleieldin Salem},
journal = {Proceedings of the Eleventh ACM Conference on Data and Application Security and Privacy},
volume = {},
pages = {},
doi = {10.1145/3422337.3447849},
arxivid = {2007.00464},
}

@article{817ccac6fb23b67487b067cd4b15404f517f93b8,
title = {AVclass2: Massive Malware Tag Extraction from AV Labels},
year = {2020},
url = {https://www.semanticscholar.org/paper/1045f411143708ecbfbbc7e4f26af58ba7715638},
abstract = {Tags can be used by malware repositories and analysis services to enable searches for samples of interest across different dimensions. Automatically extracting tags from AV labels is an efficient approach to categorize and index massive amounts of samples. Recent tools like AVclass and Euphony have demonstrated that, despite their noisy nature, it is possible to extract family names from AV labels. However, beyond the family name, AV labels contain much valuable information such as malware classes, file properties, and behaviors. This work presents AVclass2, an automatic malware tagging tool that given the AV labels for a potentially massive number of samples, extracts clean tags that categorize the samples. AVclass2 uses, and helps building, an open taxonomy that organizes concepts in AV labels, but is not constrained to a predefined set of tags. To keep itself updated as AV vendors introduce new tags, it provides an update module that automatically identifies new taxonomy entries, as well as tagging and expansion rules that capture relations between tags. We have evaluated AVclass2 on 42M samples and showed how it enables advanced malware searches and to maintain an updated knowledge base of malware concepts in AV labels.},
author = {Silvia Sebasti'an and Juan Caballero},
journal = {Annual Computer Security Applications Conference},
volume = {},
pages = {},
doi = {10.1145/3427228.3427261},
arxivid = {2006.10615},
}

@article{484bc4621cb23385e356019bf54e4ea9291114f5,
title = {Don't Pick the Cherry: An Evaluation Methodology for Android Malware Detection Methods},
year = {2019},
url = {https://www.semanticscholar.org/paper/484bc4621cb23385e356019bf54e4ea9291114f5},
abstract = {In evaluating detection methods, the malware research community relies on scan results obtained from online platforms such as VirusTotal. Nevertheless, given the lack of standards on how to interpret the obtained data to label apps, researchers hinge on their intuitions and adopt different labeling schemes. The dynamicity of VirusTotal's results along with adoption of different labeling schemes significantly affect the accuracies achieved by any given detection method even on the same dataset, which gives subjective views on the method's performance and hinders the comparison of different malware detection techniques. 
In this paper, we demonstrate the effect of varying (1) time, (2) labeling schemes, and (3) attack scenarios on the performance of an ensemble of Android repackaged malware detection methods, called dejavu, using over 30,000 real-world Android apps. Our results vividly show the impact of varying the aforementioned 3 dimensions on dejavu's performance. With such results, we encourage the adoption of a standard methodology that takes into account those 3 dimensions in evaluating newly-devised methods to detect Android (repackaged) malware.},
author = {Aleieldin Salem and Sebastian Banescu and A. Pretschner},
journal = {ArXiv},
volume = {abs/1903.10560},
pages = {},
arxivid = {1903.10560},
}

@article{ed67b98fb5654b3799668894bf57ebcda234f994,
title = {Building a Framework for Objective Evaluation of Malware Detection Methods},
year = {2019},
url = {https://www.semanticscholar.org/paper/ed67b98fb5654b3799668894bf57ebcda234f994},
abstract = {Context The research community has been working towards devising methods to detect Android malware (e.g., [7, 10, 13, 15]). Despite proving to be sometimes inconsistent [4], researchers continue to rely on VirusTotal [16] to either download training data to evaluate their newly-devised methods [14, 17, 19], or to label the apps they manually gathered from the wild (e.g., app marketplaces) [1, 6, 18], due to the lack of better, more feasible alternatives.},
author = {},
}

@article{c2ccba662b84f559a63d3110f2a5a1d7c351884b,
title = {INFERRING MALWARE DETECTOR METRICS IN THE ABSENCE OF GROUND-TRUTH},
year = {2021},
url = {https://www.semanticscholar.org/paper/c2ccba662b84f559a63d3110f2a5a1d7c351884b},
abstract = {. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . x List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi Chapter 1: Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Summary of Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Organization of the Dissertation . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Chapter 2: Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1 Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1.1 Threshold Voting and Expert Selection . . . . . . . . . . . . . . . . . . . 8 2.2 Known Ground-Truth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3 Statistical Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.4 Algorithmic Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Chapter 3: Measuring Relative Accuracy of Malware Detectors in the Absence of GroundTruthâˆ— . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Problem Statement and Methodology . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2.1 Definitions of Relative Accuracy of Malware Detectors . . . . . . . . . . . 11 3.2.2 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.3 Experiments and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.3.1 Experiments with Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . 15},
author = {John Charlton},
}

@article{63ff1eaf67e3ce76824932c19fb5ccb4bf4b69bd,
title = {A Large-scale Temporal Measurement of Android Malicious Apps: Persistence, Migration, and Lessons Learned},
year = {2021},
url = {https://www.semanticscholar.org/paper/63ff1eaf67e3ce76824932c19fb5ccb4bf4b69bd},
abstract = {We study the temporal dynamics of potentially harmful apps (PHAs) on Android by leveraging 8.8M daily on-device detections collected among 11.7M customers of a popular mobile security product between 2019 and 2020. We show that the current security model of Android, which limits security products to run as regular apps and prevents them from automatically removing malicious apps opens a significant window of opportunity for attackers. Such apps warn users about the newly discovered threats, but users do not promptly act on this information, allowing PHAs to persist on their device for an average of 24 days after they are detected. We also find that while app markets remove PHAs after these become known, there is a significant delay between when PHAs are identified and when they are removed: PHAs persist on Google Play for 77 days on average and 34 days on third party marketplaces. Finally, we find evidence of PHAs migrating to other marketplaces after being removed on the original one. This paper provides an unprecedented view of the Android PHA landscape, showing that current defenses against PHAs on Android are not as effective as commonly thought, and identifying multiple research directions that the security community should pursue, from orchestrating more effective PHA takedowns to devising better alerts for mobile security products.},
author = {Yun Shen and Pierre-Antoine Vervier and G. Stringhini},
journal = {ArXiv},
volume = {abs/2108.04754},
pages = {},
arxivid = {2108.04754},
}

@article{f2519f76d3851a42c87b933159682c5f40f739a0,
title = {Highlight and execute suspicious paths in Android malware. (Mettre en avant et exÃ©cuter les chemins suspicieux dans les malwares Android)},
year = {2018},
url = {https://www.semanticscholar.org/paper/f2519f76d3851a42c87b933159682c5f40f739a0},
abstract = {The last years have known an unprecedented growth in the use of mobile devices especially smartphones. They became omnipresent in our daily life because of the features they offer. They allow the user to install third-party apps to achieve numerous tasks. Smartphones are mostly governed by the Android operating system. It is today installed on more than 80% of the smartphones. Mobile apps collect a huge amount of data such as email addresses, contact list, geolocation, photos and bank account credentials. Consequently, Android has become a favorable target for cyber criminals. Thus, understanding the issue, i.e., how Android malware operates and how to detect it, became an important research challenge. Android malware frequently tries to bypass static analysis using multiple techniques such as code obfuscation and dynamic code loading. To overcome these limitations, many analysis techniques have been proposed to execute the app and monitor its behavior at runtime. Nevertheless, malware developers use time and logic bombs to prevent the malicious code from executing except under certain circumstances. Therefore, more actions are needed to trigger it and monitor its behavior. Recent approaches try to automatically characterize the malicious behavior by identifying the most suspicious locations in the code and forcing them to execute. They strongly rely on the computation of application global control flow graphs (CFGs). However, these CFGs are incomplete because they do not take into consideration all types of execution paths. These approaches solely analyze the application code and miss the execution paths that occur when the application calls a framework method that in turn calls another application method. We propose in this dissertation a tool, GPFinder, that automatically exhibits execution paths towards suspicious locations in the code by computing global CFGs that include edges representing explicit and implicit interprocedural calls. It also gives key information about the analyzed application in order to understand how the suspicious code was injected into the application. To validate our approach, we use GPFinder to study a collection of 14,224 malware samples, and we evaluate that 72.69% of the samples have at least one suspicious code location which is only reachable through implicit calls. Triggering approaches mainly use one of the following strategies to run a specific portion of the application's code: the first approach heavily modifies the app to launch the targeted code without keeping the original behavioral context. The second approach generates the input to force the execution flow to take the desired path without modifying the app's code. However, it is sometimes hard to launch a specific code location just by fuzzing the input. For instance, when the application performs a hash on the input data and compares the result to a fixed string to decide which branch of the condition to take, the fuzzing program should reverse the hashing function, which is obviously a hard problem. We propose in this dissertation a tool, TriggerDroid, that has a twofold goal: force the execution of the suspicious code and keep its context close to the original one. It crafts the required framework events to launch the right app component and satisfies the necessary triggering conditions to take the desired execution path. To validate our approach, we led an experiment on a dataset of 135 malware samples from 71 different families. Results show that our approach needs more refinement and adaptation to handle special cases due to the highly diverse malware dataset that we analyzed. Finally, we give a feedback on the experiments we led on different malware datasets, and we explain our experimental process. Finally, we present the Kharon dataset, a collection of well documented Android malware that can be used to understand the malware landscape.},
author = {Mourad Leslous},
}

@article{eb7f0d2a9256a9b7e875cf7107d75dff2bfb61bb,
title = {Effective dataset construction method using Dexofuzzy based on Android malware opcode mining},
year = {2021},
url = {https://www.semanticscholar.org/paper/eb7f0d2a9256a9b7e875cf7107d75dff2bfb61bb},
abstract = {},
author = {Shinho Lee and Wookhyun Jung and Wonrak Lee and HyungGeun Oh and Eui Tak Kim},
journal = {ICT Express},
volume = {},
pages = {},
doi = {10.1016/j.icte.2021.10.001},
}
