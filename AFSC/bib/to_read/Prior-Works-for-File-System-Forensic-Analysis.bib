@article{c6679987a9b5e8672bec243caaf7b3f4ed7fca55,
title = {Carving contiguous and fragmented files with fast object validation},
year = {2007},
url = {https://www.semanticscholar.org/paper/c6679987a9b5e8672bec243caaf7b3f4ed7fca55},
abstract = {''File carving'' reconstructs files based on their content, rather than using metadata that points to the content. Carving is widely used for forensics and data recovery, but no file carvers can automatically reassemble fragmented files. We survey files from more than 300 hard drives acquired on the secondary market and show that the ability to reassemble fragmented files is an important requirement for forensic work. Next we analyze the file carving problem, arguing that rapid, accurate carving is best performed by a multi-tier decision problem that seeks to quickly validate or discard candidate byte strings - ''objects'' - from the media to be carved. Validators for the JPEG, Microsoft OLE (MSOLE) and ZIP file formats are discussed. Finally, we show how high speed validators can be used to reassemble fragmented files.},
author = {S. Garfinkel},
journal = {Digit. Investig.},
volume = {4},
pages = {2-12},
doi = {10.1016/J.DIIN.2007.06.017},
}

@article{c81d7162ef4dcd853033fcdf516b156ce08fcbf5,
title = {Scalpel: A Frugal, High Performance File Carver},
year = {2005},
url = {https://www.semanticscholar.org/paper/c81d7162ef4dcd853033fcdf516b156ce08fcbf5},
abstract = {File carving is an important technique for digital forensics investigation and for simple data recovery. By using a database of headers and footers (essentially, strings of bytes at predictable offsets) for specific file types, file carvers can retrieve files from raw disk images, regardless of the type of filesystem on the disk image. Perhaps more importantly, file carving is possible even if the filesystem metadata has been destroyed. This paper presents some requirements for high performance file carving, derived during design and implementation of Scalpel, a new open source file carving application. Scalpel runs on machines with only modest resources and performs carving operations very rapidly, outperforming most, perhaps all, of the current generation of carving tools. The results of a number of experiments are presented to support this assertion.},
author = {G. Richard and Vassil Roussev},
}

@article{762151ba1534960fcc11e3548ff8361ee2917382,
title = {File Type Identification of Data Fragments by Their Binary Structure},
year = {2006},
url = {https://www.semanticscholar.org/paper/762151ba1534960fcc11e3548ff8361ee2917382},
abstract = {Rapidly gaining information superiority is vital when fighting an enemy, but current computer forensics tools, which require file headers or a working file system to function, do not enable us to quickly map out the contents of corrupted hard disks or other fragmented storage media found at crime scenes. The lack of proper tools slows down the hunt for information, which would otherwise help in gaining the upper hand against IT based perpetrators. To address this problem, this paper presents an algorithm which allows categorization of data fragments based solely on their structure, without the need for any meta data. The algorithm is based on measuring the rate of change of the byte contents of digital media and extends the byte frequency distribution based Oscar method presented in an earlier paper. The evaluation of the new method shows a detection rate of 99.2 %, without generating any false positives, when used to scan for JPEG data. The slowest implementation of the algorithm scans a 72.2 MB file in approximately 2.5 seconds and scales linearly},
author = {M. Karresand and N. Shahmehri},
journal = {2006 IEEE Information Assurance Workshop},
volume = {},
pages = {140-147},
doi = {10.1109/IAW.2006.1652088},
}

@article{660bc36278483acf080f4cc34b56fbf5f80b36e6,
title = {Oscar - File Type Identification of Binary Data in Disk Clusters and RAM Pages},
year = {2006},
url = {https://www.semanticscholar.org/paper/660bc36278483acf080f4cc34b56fbf5f80b36e6},
abstract = {This paper proposes a method, called Oscar, for determining the probable file type of binary data fragments. The Oscar method is based on building models, called centroids, of the mean and standard deviation of the byte frequency distribution of different file types. A weighted quadratic distance metric is then used to measure the distance between the centroid and sample data fragments. If the distance falls below a threshold, the sample is categorized as probably belonging to the modelled file type. Oscar is tested using JPEG pictures and is shown to give a high categorization accuracy, i.e. high detection rate and low false positives rate. By using a practical example we demonstrate how to use the Oscar method to prove the existence of known pictures based on fragments of them found in RAM and the swap partition of a computer.},
author = {M. Karresand and N. Shahmehri},
doi = {10.1007/0-387-33406-8_35},
}

@article{6346bf107065eec4bcce590465136e01bfe42ae7,
title = {File System Forensic Analysis},
year = {2005},
url = {https://www.semanticscholar.org/paper/6346bf107065eec4bcce590465136e01bfe42ae7},
abstract = {The Definitive Guide to File System Analysis: Key Concepts and Hands-on TechniquesMost digital evidence is stored within the computer's file system, but understanding how file systems work is one of the most technically challenging concepts for a digital investigator because there exists little documentation. Now, security expert Brian Carrier has written the definitive reference for everyone who wants to understand and be able to testify about how file system analysis is performed.Carrier begins with an overview of investigation and computer foundations and then gives an authoritative, comprehensive, and illustrated overview of contemporary volume and file systems: Crucial information for discovering hidden evidence, recovering deleted data, and validating your tools. Along the way, he describes data structures, analyzes example disk images, provides advanced investigation scenarios, and uses today's most valuable open source file system analysis tools-including tools he personally developed. Coverage includes Preserving the digital crime scene and duplicating hard disks for "dead analysis" Identifying hidden data on a disk's Host Protected Area (HPA) Reading source data: Direct versus BIOS access, dead versus live acquisition, error handling, and more Analyzing DOS, Apple, and GPT partitions; BSD disk labels; and Sun Volume Table of Contents using key concepts, data structures, and specific techniques Analyzing the contents of multiple disk volumes, such as RAID and disk spanning Analyzing FAT, NTFS, Ext2, Ext3, UFS1, and UFS2 file systems using key concepts, data structures, and specific techniques Finding evidence: File metadata, recovery of deleted files, data hiding locations, and more Using The Sleuth Kit (TSK), Autopsy Forensic Browser, and related open source toolsWhen it comes to file system analysis, no other book offers this much detail or expertise. Whether you're a digital forensics specialist, incident response team member, law enforcement officer, corporate security specialist, or auditor, this book will become an indispensable resource for forensic investigations, no matter what analysis tools you use.Brian Carrier has authored several leading computer forensic tools, including The Sleuth Kit (formerly The @stake Sleuth Kit) and the Autopsy Forensic Browser. He has authored several peer-reviewed conference and journal papers and has created publicly available testing images for forensic tools. Currently pursuing a Ph.D. in Computer Science and Digital Forensics at Purdue University, he is also a research assistant at the Center for Education and Research in Information Assurance and Security (CERIAS) there. He formerly served as a research scientist at @stake and as the lead for the @stake Response Team and Digital Forensic Labs. Carrier has taught forensics, incident response, and file systems at SANS, FIRST, the @stake Academy, and SEARCH.Brian Carrier's http://www.digital-evidence.org contains book updates and up-to-date URLs from the book's references.Â© Copyright Pearson Education. All rights reserved.},
author = {Brian D. Carrier},
}

@article{198f2f0cec088197cf39a979f95554a63844f776,
title = {Content based file type detection algorithms},
year = {2003},
url = {https://www.semanticscholar.org/paper/198f2f0cec088197cf39a979f95554a63844f776},
abstract = {Identifying the true type of a computer file can be a difficult problem. Previous methods of file type recognition include fixed file extensions, fixed "magic numbers" stored with the files, and proprietary descriptive file wrappers. All of these methods have significant limitations. This paper proposes algorithms for automatically generating "fingerprints" of file types based on a set of known input files, then using the fingerprints to recognize the true type of unknown files based on their content, rather than metadata associated with them. Recognition is performed by three different algorithms based on: byte frequency analysis, byte frequency cross-correlation analysis, and file header/trailer analysis. Tests were run to measure the accuracy of these algorithms. The accuracy varied from 23% to 96% depending upon which algorithm was used. These algorithms could be used by virus scanning packages, firewalls, intrusion detection systems, forensic analyses of computer hard drives, Web browsers, or any other program that needs to identify the types of files for proper operation. File type detection is also important to the operating systems for correct identification and handling of files regardless of file extension.},
author = {Mason McDaniel and M. H. Heydari},
journal = {36th Annual Hawaii International Conference on System Sciences, 2003. Proceedings of the},
volume = {},
pages = {10 pp.-},
doi = {10.1109/HICSS.2003.1174905},
}

@article{ee68d4b6eaa4d07bb7f64410b511bf5bb2dfe182,
title = {Fileprints: identifying file types by n-gram analysis},
year = {2005},
url = {https://www.semanticscholar.org/paper/ee68d4b6eaa4d07bb7f64410b511bf5bb2dfe182},
abstract = {We propose a method to analyze files to categorize their type using efficient 1-gram analysis of their binary contents. Our aim is to be able to accurately identify the true type of an arbitrary file using statistical analysis of their binary contents without parsing. Consequently, we may determine the type of a file if its name does not announce its true type. The method represents each file type by a compact representation we call a fileprint, effectively a simple means of representing all members of the same file type by a set of statistical 1-gram models. The method is designed to be highly efficient so that files can be inspected with little or no buffering, and on a network appliance operating in high bandwidth environment or when streaming the file from or to disk.},
author = {Wei-Jen Li and Ke Wang and S. Stolfo and B. Herzog},
journal = {Proceedings from the Sixth Annual IEEE SMC Information Assurance Workshop},
volume = {},
pages = {64-71},
doi = {10.1109/IAW.2005.1495935},
}

@article{e2fbdab8f1694455da8259491e83462c4a9dccd1,
title = {Statistical Disk Cluster Classification for File Carving},
year = {2007},
url = {https://www.semanticscholar.org/paper/e2fbdab8f1694455da8259491e83462c4a9dccd1},
abstract = {File carving is the process of recovering files from a disk without the help of a file system. In forensics, it is a helpful tool in finding hidden or recently removed disk content. Known signatures in file headers and footers are especially useful in carving such files out, that is, from header until footer. However, this approach assumes that file clusters remain in order. In case of file fragmentation, file clusters can be disconnected and the order can even be disrupted such that straighforward carving will fail. In this paper, we focus on methods for classifying clusters into file types by using the statistics of the clusters. By not exploiting the possible embedded signatures, we generate evidence from a different source that can be integrated later on. We propose a set of characteristic features and use statistical pattern recognition to learn a supervised classification model for a range of relevant file types. We exploit the statistics of a restricted number of neighboring clusters (context) to improve classification performance. In the experiments we show that the proposed features indeed enable the differentation of clusters into file types. Moreover, for some file types the incorporation of cluster context improves the recognition performance significantly.},
author = {C. Veenman},
journal = {Third International Symposium on Information Assurance and Security},
volume = {},
pages = {393-398},
doi = {10.1109/IAS.2007.75},
}

@article{c50d682c000988d9b0af10fb41f59cf738c8e5a2,
title = {Automated reassembly of file fragmented images using greedy algorithms},
year = {2006},
url = {https://www.semanticscholar.org/paper/c50d682c000988d9b0af10fb41f59cf738c8e5a2},
abstract = {The problem of restoring deleted files from a scattered set of fragments arises often in digital forensics. File fragmentation is a regular occurrence in hard disks, memory cards, and other storage media. As a result, a forensic analyst examining a disk may encounter many fragments of deleted digital files, but is unable to determine the proper sequence of fragments to rebuild the files. In this paper, we investigate the specific case where digital images are heavily fragmented and there is no file table information by which a forensic analyst can ascertain the correct fragment order to reconstruct each image. The image reassembly problem is formulated as a k-vertex disjoint graph problem and reassembly is then done by finding an optimal ordering of fragments. We provide techniques for comparing fragments and describe several algorithms for image reconstruction based on greedy heuristics. Finally, we provide experimental results showing that images can be reconstructed with high accuracy even when there are thousands of fragments and multiple images involved.},
author = {N. Memon and Anindrabatha Pal},
journal = {IEEE Transactions on Image Processing},
volume = {15},
pages = {385-393},
doi = {10.1109/TIP.2005.863054},
pmid = {16479808},
}

@article{a26c48cff9aaed7adea0fd7176ff358670c8474c,
title = {Detecting file fragmentation point using sequential hypothesis testing},
year = {2008},
url = {https://www.semanticscholar.org/paper/a26c48cff9aaed7adea0fd7176ff358670c8474c},
abstract = {File carving is a technique whereby data files are extracted from a digital device without the assistance of file tables or other disk meta-data. One of the primary challenges in file carving can be found in attempting to recover files that are fragmented. In this paper, we show how detecting the point of fragmentation of a file can benefit fragmented file recovery. We then present a sequential hypothesis testing procedure to identify the fragmentation point of a file by sequentially comparing adjacent pairs of blocks from the starting block of a file until the fragmentation point is reached. By utilizing serial analysis we are able to minimize the errors in detecting the fragmentation points. The performance results obtained from the fragmented test-sets of DFRWS 2006 and 2007 show that the method can be effectively used in recovery of fragmented files.},
author = {A. Pal and H. Sencar and N. Memon},
journal = {Digit. Investig.},
volume = {5},
pages = {S2-S13},
doi = {10.1016/J.DIIN.2008.05.015},
}
