{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab  – Linear Regression\n",
    "## PART 1 Univariable Linear Regression\n",
    "\n",
    "Objectives: Implement linear regression with one variable (feature) and get to see it works on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab work you will implement Univariate Linear Regression to predict profits for a food truck (a large vehicle equipped to cook and sell food). Suppose you are the CEO of a restaurant franchise and consider different cities for opening a new food truck. The chain already has trucks in various cities and you have data for profits and populations from the cities. You would like to use this data to help you select in which city to expand your business.\n",
    "\n",
    "First, import all relevant libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Plot the Data. \n",
    "\n",
    "The file *Uni_linear.txt* contains the dataset for the Linear Regression problem. The first column is the population of a city (variable X) and the second column is the profit of a food truck in that city (variable y). The values are scaled: number of people/10000 and profit in dolars/10000. A negative value for profit indicates a loss. \n",
    "\n",
    "Load data into the variable **data** (using function pd.read_csv from panda library).\n",
    "\n",
    "Create a scatter plot of data similar to Fig.1 (using plt.scatter). \n",
    "\n",
    "<img src=\"images/f1.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 1** : **file Uni_linear.txt** </center></caption>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Uni_linear.txt\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a few examples from the dataset \n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Fig. 1  (using plt.scatter)\n",
    "plt.scatter(data[0],data[1])\n",
    "\n",
    "#Add labels : plt.xlabel; plt.ylabel; plt.title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Cost Function $J(\\theta)$\n",
    "\n",
    "The objective of Linear Regression is to minimize the cost function: $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)} )^2$\n",
    "\n",
    "where *h* is the linear model: $h_\\theta(x)=\\theta^Tx=\\theta_0+\\theta_1x_1$ \n",
    "\n",
    "Complete the function **computeCost(X,y,theta)**.  Remember that the variables X and y are not scalar values, X is an array (matrix) with dimension (*mx2*), y is an array (vector) with dimension (*mx1*), *m* represent the examples from the training set.\n",
    "\n",
    "*np.dot()* - generates the vectorized dot product \n",
    "\n",
    "*np.sum()* - computes the sum of errors over all given examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X,y,theta):\n",
    "    \"\"\"\n",
    "   Take the numpy arrays X, y, theta and return the cost function J for this theta. \n",
    "\n",
    "    \"\"\"\n",
    "    #number of training examples \n",
    "    m= ?\n",
    "    h=np.dot(X,theta)\n",
    "    square_err=(h - y)**2\n",
    "    J=1/(2*m) * np.sum(square_err)\n",
    "\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will provide values for the arguments of **computeCost(X,y,theta)**. \n",
    "First, extract X and y from data. \n",
    "\n",
    "Check if X and y are rank 1 arrays (m,), and if yes, you need to reshape them to be 2-dimensonal arrays (m,1).  \n",
    "Each example is stored as a row.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n=data.values # extract only the values of the two columns from the dataFrame data\n",
    "X= ?\n",
    "y= ?\n",
    "\n",
    "#number of training examples\n",
    "m=len(y)\n",
    "\n",
    "#To take into account the intercept term theta_0, we add an extra first column to X and \n",
    "#set it to all ones (np.ones). This allows to treat  theta_0 as simply another ‘feature parameter’. \n",
    "\n",
    "X=np.append(np.ones((m,1)),X,axis=1)  #axis=0 are the rows, axis=1 are the columns\n",
    "\n",
    "#Initialize the fitting parameters theta to 0 (np.zeros)\n",
    "\n",
    "theta=\n",
    "\n",
    "#You should see a cost of about 32.07.\n",
    "\n",
    "computeCost(X,y,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Minimize the cost function $J(\\theta)$ by updating Equation        \n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$ (simultaneously update $\\theta_j$ for all $j$) and repeat unitil convergence. \n",
    "\n",
    "\n",
    "Implement gradient descent in the function **gradientDescent**. The loop structure is written, you need to supply the updates to $\\theta$  within each iteration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,num_iters):\n",
    "    \"\"\"\n",
    "    Take numpy arrays X, y and theta and update theta by taking num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "     Return: theta and the list of the cost of theta (J_history) during each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    m=len(y)\n",
    "    J_history=[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        h=np.dot(X,theta)\n",
    "        grad = np.dot(X.transpose(),(h-y)) #Vectorized way to compute all gradients simultaneously \n",
    "        descent=(alpha/m) * grad\n",
    "        theta= theta - descent\n",
    "        \n",
    "        J_history.append(computeCost(X,y,theta))\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run  **gradientDescent** with learning rate alpha= 0.01 and 1500 iterations and get the final parameters $\\theta_0$ = -3.630, $\\theta_1$=1.166. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta,J_history = gradientDescent(?,?,....)\n",
    "\n",
    "print(\"h(x) =\"+str(round(theta[0,0],2))+\" + \"+str(round(theta[1,0],2))+\"x1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Cost Function $J(\\theta)$ \n",
    "To understand the cost function $J(\\theta)$ better, we plot the cost in a 3D graph over a grid of values for $\\theta_0$ and $\\theta_1$. The cost function has a global minimum. This minimum is the optimal point for $\\theta_0$ and $\\theta_1$ and each step of gradient descent moves closer to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating values for theta0, theta1 and the resulting cost value\n",
    "theta0_vals=np.linspace(-10,10,100)\n",
    "theta1_vals=np.linspace(-1,4,100)\n",
    "J_vals=np.zeros((len(theta0_vals),len(theta1_vals)))\n",
    "\n",
    "for i in range(len(theta0_vals)):\n",
    "    for j in range(len(theta1_vals)):\n",
    "        t=np.array([theta0_vals[i],theta1_vals[j]])\n",
    "        J_vals[i,j]=computeCost(X,y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the surface plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf=ax.plot_surface(theta0_vals,theta1_vals,J_vals,cmap=\"coolwarm\")\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_xlabel(\"$\\Theta_0$\")\n",
    "ax.set_ylabel(\"$\\Theta_1$\")\n",
    "ax.set_zlabel(\"$J(\\Theta)$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the implementation\n",
    "\n",
    "A good way to verify that gradient descent is working correctly is to plot $J(\\theta)$ against the number of iteration. Function **gradientDescent** calls function **computeCost** on every iteration and saves the costs over the iterations. If the algorithm works properly, $J(\\theta)$ should never increase, and should converge to a steady value. Plot the gradient history (use *plt.plot()*) and get a curve similar to Fig.2. \n",
    "\n",
    "<img src=\"images/f2.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig.2** : **$J(\\theta)$** </center></caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(?)\n",
    "\n",
    "#Add labels : plt.xlabel; plt.ylabel; plt.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph with Best Line Fit \n",
    "\n",
    "Overlap data and the best line fit (with the optimized  $\\theta$ values) as shown in Fig.3. \n",
    "\n",
    "<img src=\"images/f3.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 3** : ** ** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data (the same as Fig. 1)\n",
    "plt.scatter(?,?)\n",
    "\n",
    "#add the best line fit with red colour\n",
    "x_fit=range(25)\n",
    "y_fit=theta[0]+theta[1]*x_fit\n",
    "\n",
    "plt.plot(x_fit,y_fit,color=\"r\")\n",
    "plt.xlabel(\"Population of City (10,000s)\")\n",
    "plt.ylabel(\"Profit ($10,000\")\n",
    "plt.title(\"Profit Prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions  using the optimized $\\theta$ values\n",
    "\n",
    "Complete function **predict** to compute model predictions: $h_\\theta(x) = X*theta$. Apply vectorized computations with np.dot(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,theta):\n",
    "    \"\"\"\n",
    "    Takes in numpy array of x and theta and return the predicted value of y based on theta\n",
    "    \"\"\"\n",
    "    \n",
    "    h= ?\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run  **predict** to predict profits in areas of 35,000 and 70,000 people. Note that you need to scale the numbers properly !\n",
    "\n",
    "Answer: \n",
    "\n",
    "        For population = 35,000, predicted profit of 4520 USD\n",
    "\n",
    "        For population = 70,000, predicted profit of 45342 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict1=predict([1,3.5],theta)*10000\n",
    "print(\"For population = 35,000, we predict a profit of $: \") \n",
    "print ( predict1 )\n",
    "\n",
    "#predict1 has to be a scalar to be transformed into string \n",
    "print(\"For population = 35,000, we predict a profit of $\"+str(round(predict1[0],0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict2= ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PART 2 Multivariable Linear Regression\n",
    "Objectives: Implement linear regression with multiple variables (features) and get to see it works on data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "\n",
    "The file **Multi_linear.txt** contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house.  \n",
    "\n",
    "Load the data into the array data2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a few examples from the dataset \n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some statistics\n",
    "data2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n",
    "\n",
    "Note that house sizes are much larger values (about 1000 times) than the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly. \n",
    "To make sure features are on a similar scale apply Mean normalization.\n",
    "\n",
    "$x_i = \\frac{x_i - \\mu_i}{\\sigma_i}$\n",
    "\n",
    "Your task is to complete the code in function **featureNormalization(X)**:\n",
    "\n",
    "• Compute the mean value  $\\mu_i$ of each feature (use np.mean(X,axis=0)) \n",
    "\n",
    "• compute the standard deviation $\\sigma_i$ of each feature (use np.std(X,axis=0)) \n",
    "\n",
    "• Apply the equation above.\n",
    "\n",
    "**Remark:** When normalizing the features, it is important to store the mean value and the standard deviation used for normalization. After optimizing the parameters of the model, you want to predict the price of a new example not seen before.\n",
    "You must first normalize the features of that new example using the mean and standard deviation previously computed from the training set.\n",
    "\n",
    "**Remark:** Mean normalization is an alternative to normalizing by making the absolute values < 1 (i.e. dividing by MaxValue-MinValue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalization(X):\n",
    "    \"\"\"\n",
    "    Take in numpy array of X values and return normalize X values,\n",
    "    the mean and standard deviation of each feature\n",
    "    \"\"\"\n",
    "    mean=np.mean(X,axis=0)  #axis=0 are the rows, axis=1 are the columns\n",
    "    std=np.std(X,axis=0)\n",
    "    \n",
    "    X_norm = (X - mean)/std\n",
    "    \n",
    "    return X_norm , mean , std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract from data2 the features in X2 and the output in y2. If rank 1, reshape them to have 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n2=data2.values\n",
    "X2 =?\n",
    "y2=?\n",
    "\n",
    "#Run featureNormalization to normalize X2, store the means and stds.\n",
    "X2, mean_X2, std_X2 = ?\n",
    "\n",
    "#After normalizing the features add an extra column of 1's corresponding to x0 = 1. \n",
    "\n",
    "X2=np.append(.....)\n",
    "\n",
    "\n",
    "#Inicialize the vector of model parameters theta2 = 0.\n",
    "theta2= ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost $J(\\theta)$\n",
    "In the previous (univariate) problem you have implemented the functions **computeCost** and **gradientDescent** in a vectorized way, therefore they will work for linear regression with any number of features. \n",
    "\n",
    "Answer: Cost = 65591548106.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeCost(X2,y2,theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "Apply **gradientDescent** with different learning rates (e.g. alpha=[0.001, 0.01, 0.1, 0.3 1.4]) and 400 iterations.\n",
    "\n",
    "You may need to adjust the number of iterations in order to see well the overall trend in $J(\\theta)$ curve below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicialize theta2 = 0\n",
    "theta2= ?\n",
    "theta2, J_history2 = gradientDescent(?,?,?,?,?)\n",
    "print(\"h(x) =\"+str(round(theta2[0,0],2))+\" + \"+str(round(theta2[1,0],2))+\"x1 + \"+str(round(theta2[2,0],2))+\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Cost Function $J(\\theta)$\n",
    "\n",
    "If the learning rate is too small (e.g. 0.001), the gradient descent takes a very long time to converge to the optimal value. \n",
    "\n",
    "If the learning rate is too large (e.g. 1.4), $J(\\theta)$ can diverge and \"blow up\", resulting in values which are too large for computer calculations. In these situations, Python will return nan (not a number). This is often caused by undefined operations that involve +/- infinity.\n",
    "\n",
    "Get a similar plot as in Fig.4. \n",
    "\n",
    "<img src=\"images/f4.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 4** Cost function for different learning rates ** ** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions using the optimized $\\theta$ values\n",
    "\n",
    "Using the best learning rate you found, run gradient descent until convergence to find the optimal $\\theta$ values.\n",
    "\n",
    "Predict the price of a house with 1650 square feet and 3 bedrooms (use function predict you have implemented in Part 1). \n",
    "\n",
    "Don't forget to normalize the features, before making this prediction!\n",
    "\n",
    "Answer: the price is about $293000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample =?\n",
    "#feature normalisation of x_sample\n",
    "x_sample= ?\n",
    "#add 1\n",
    "x_sample=\n",
    "predict3=predict(?,?)\n",
    "print(\"For size of house = 1650, Number of bedroom = 3, we predict a house value of $\"+str(round(predict3,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
