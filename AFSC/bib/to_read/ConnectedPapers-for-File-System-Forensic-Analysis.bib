@article{6346bf107065eec4bcce590465136e01bfe42ae7,
title = {File System Forensic Analysis},
year = {2005},
url = {https://www.semanticscholar.org/paper/6346bf107065eec4bcce590465136e01bfe42ae7},
abstract = {The Definitive Guide to File System Analysis: Key Concepts and Hands-on TechniquesMost digital evidence is stored within the computer's file system, but understanding how file systems work is one of the most technically challenging concepts for a digital investigator because there exists little documentation. Now, security expert Brian Carrier has written the definitive reference for everyone who wants to understand and be able to testify about how file system analysis is performed.Carrier begins with an overview of investigation and computer foundations and then gives an authoritative, comprehensive, and illustrated overview of contemporary volume and file systems: Crucial information for discovering hidden evidence, recovering deleted data, and validating your tools. Along the way, he describes data structures, analyzes example disk images, provides advanced investigation scenarios, and uses today's most valuable open source file system analysis tools-including tools he personally developed. Coverage includes Preserving the digital crime scene and duplicating hard disks for "dead analysis" Identifying hidden data on a disk's Host Protected Area (HPA) Reading source data: Direct versus BIOS access, dead versus live acquisition, error handling, and more Analyzing DOS, Apple, and GPT partitions; BSD disk labels; and Sun Volume Table of Contents using key concepts, data structures, and specific techniques Analyzing the contents of multiple disk volumes, such as RAID and disk spanning Analyzing FAT, NTFS, Ext2, Ext3, UFS1, and UFS2 file systems using key concepts, data structures, and specific techniques Finding evidence: File metadata, recovery of deleted files, data hiding locations, and more Using The Sleuth Kit (TSK), Autopsy Forensic Browser, and related open source toolsWhen it comes to file system analysis, no other book offers this much detail or expertise. Whether you're a digital forensics specialist, incident response team member, law enforcement officer, corporate security specialist, or auditor, this book will become an indispensable resource for forensic investigations, no matter what analysis tools you use.Brian Carrier has authored several leading computer forensic tools, including The Sleuth Kit (formerly The @stake Sleuth Kit) and the Autopsy Forensic Browser. He has authored several peer-reviewed conference and journal papers and has created publicly available testing images for forensic tools. Currently pursuing a Ph.D. in Computer Science and Digital Forensics at Purdue University, he is also a research assistant at the Center for Education and Research in Information Assurance and Security (CERIAS) there. He formerly served as a research scientist at @stake and as the lead for the @stake Response Team and Digital Forensic Labs. Carrier has taught forensics, incident response, and file systems at SANS, FIRST, the @stake Academy, and SEARCH.Brian Carrier's http://www.digital-evidence.org contains book updates and up-to-date URLs from the book's references.© Copyright Pearson Education. All rights reserved.},
author = {Brian D. Carrier},
}

@article{a8ac2b42c8968a2149ab4e1113c19207e5f45411,
title = {A File Carving Algorithm for Digital Forensics},
year = {2009},
url = {https://www.semanticscholar.org/paper/a8ac2b42c8968a2149ab4e1113c19207e5f45411},
abstract = {The File-Carving algorithm when doing file rehabilitation, the part is which important. "Carving" is the term most often used to indicate the act of recovering a file from unstructured digital forensic images. Until present time the file caring algorithm which becomes known the file is continuous, n case it knows Header and Footer information the file carving which is accurate becomes accomplished. The present paper the hazard which amends the problem point where the original broad way rehabilitation does not become a file and it amends a problem point design.},
author = {Deok-Gyu Park and Sangjoon Park and Jongchan Lee and Si-Young No and Seong-Yoon Shin},
doi = {10.1007/978-3-642-02454-2_45},
}

@article{1e1d2a5643d70f588fe91f5f9bcd9e0213effa85,
title = {Fragmentation Point Detection of JPEG Images at DHT Using Validator},
year = {2009},
url = {https://www.semanticscholar.org/paper/1e1d2a5643d70f588fe91f5f9bcd9e0213effa85},
abstract = {File carving is an important, practical technique for data recovery in digital forensics investigation and is particularly useful when filesystem metadata is unavailable or damaged. The research on reassembly of JPEG files with RST markers, fragmented within the scan area have been done before. However, fragmentation within Define Huffman Table (DHT) segment is yet to be resolved. This paper analyzes the fragmentation within the DHT area and list out all the fragmentation possibilities. Two main contributions are made in this paper. Firstly, three fragmentation points within DHT area are listed. Secondly, few novel validators are proposed to detect these fragmentations. The result obtained from tests done on manually fragmented JPEG files, showed that all three fragmentation points within DHT are successfully detected using validators.},
author = {Kamaruddin Malik Mohamad and M. M. Deris},
doi = {10.1007/978-3-642-10509-8_20},
}

@article{29367c7fcb41cecf33dd66a985b0b1d628331e0e,
title = {Secure E-Commerce Protocol for Purchase of e-Goods - Using Smart Card},
year = {2007},
url = {https://www.semanticscholar.org/paper/29367c7fcb41cecf33dd66a985b0b1d628331e0e},
abstract = {The rapid growth of e-commerce for buying and selling electronic goods or intangible goods has necessitated the need for development of a number of e-commerce protocols, which ensure integrity, confidentiality, atomicity and fair exchange. In this paper we propose an e-commerce protocol for performing business over the Internet. The protocol uses a smart card for ensuring mutual authentication, dispute resolution and fair exchange and reduces reliance on a trusted third party. Atomicity is also maintained in the protocol.},
author = {S. Devane and M. Chatterjee and D. Phatak},
doi = {10.1109/IAS.2007.75},
}

@article{198f2f0cec088197cf39a979f95554a63844f776,
title = {Content based file type detection algorithms},
year = {2003},
url = {https://www.semanticscholar.org/paper/198f2f0cec088197cf39a979f95554a63844f776},
abstract = {Identifying the true type of a computer file can be a difficult problem. Previous methods of file type recognition include fixed file extensions, fixed "magic numbers" stored with the files, and proprietary descriptive file wrappers. All of these methods have significant limitations. This paper proposes algorithms for automatically generating "fingerprints" of file types based on a set of known input files, then using the fingerprints to recognize the true type of unknown files based on their content, rather than metadata associated with them. Recognition is performed by three different algorithms based on: byte frequency analysis, byte frequency cross-correlation analysis, and file header/trailer analysis. Tests were run to measure the accuracy of these algorithms. The accuracy varied from 23% to 96% depending upon which algorithm was used. These algorithms could be used by virus scanning packages, firewalls, intrusion detection systems, forensic analyses of computer hard drives, Web browsers, or any other program that needs to identify the types of files for proper operation. File type detection is also important to the operating systems for correct identification and handling of files regardless of file extension.},
author = {Mason McDaniel and M. H. Heydari},
journal = {36th Annual Hawaii International Conference on System Sciences, 2003. Proceedings of the},
volume = {},
pages = {10 pp.-},
doi = {10.1109/HICSS.2003.1174905},
}

@article{194ae6dc9371ba166a0de493c2f62bac5181c011,
title = {Automatic reassembly of document fragments via context based statistical models},
year = {2003},
url = {https://www.semanticscholar.org/paper/194ae6dc9371ba166a0de493c2f62bac5181c011},
abstract = {Reassembly of fragmented objects from a collection of randomly mixed fragments is a common problem in classical forensics. We address the digital forensic equivalent, i.e., reassembly of document fragments, using statistical modelling tools applied in data compression. We propose a general process model for automatically analyzing a collection fragments to reconstruct the original document by placing the fragments in proper order. Probabilities are assigned to the likelihood that two given fragments are adjacent in the original using context modelling techniques in data compression. The problem of finding the optimal ordering is shown to be equivalent to finding a maximum weight Hamiltonian path in a complete graph. Heuristics are designed and explored and implementation results provided which demonstrate the validity of the proposed technique.},
author = {K. Shanmugasundaram and N. Memon},
journal = {19th Annual Computer Security Applications Conference, 2003. Proceedings.},
volume = {},
pages = {152-159},
doi = {10.1109/CSAC.2003.1254320},
}

@article{424daafd9ac88cf67efd046d02ed1eed4f65fd41,
title = {Defining Digital Forensic Examination and Analysis Tool Using Abstraction Layers},
year = {2003},
url = {https://www.semanticscholar.org/paper/424daafd9ac88cf67efd046d02ed1eed4f65fd41},
abstract = {This paper uses the theory of abstraction layers to describe the purpose and goals of digital forensic analysis tools. Using abstraction layers, we identify where tools can introduce errors and provide requirements that the tools must follow. Categories of forensic analysis types are also defined based on the abstraction layers. Abstraction layers are not a new concept, but their usage in digital forensic analysis is not well documented. What does it mean to be a Digital Forensic Analysis Tool? How do we categorize the different types of analysis tools? For example, an investigator can view the files and directories of a suspect system by using either specialized forensic software or by using the operating system (OS) of an analysis system and viewing the files by mounting the drive. Both methods allow the investigator to view evidence in allocated files, but only the specialized forensic software allows him to easily view unallocated files. Additional tools are required if he is relying on the OS. Clearly both allow the investigator to find evidence and therefore should be considered forensic tools, but it is unclear how we should compare and categorize them. The high-level process of digital forensics includes the acquisition of data from a source, analysis of the data and extraction of evidence, and preservation and presentation of the evidence. Previous work has been done on the theory and requirements of data acquisition [7] and the preservation of evidence [4]. This paper addresses the tools that are used for the analysis of data and extraction of evidence. This paper examines the nature of tools in digital forensics and proposes definitions and requirements. Current digital forensic tools produce results that have been successfully used in prosecutions, but lack designs that were created with forensic science needs. They provide the investigator with access to evidence, but typically do not provide access to methods for verifying that the evidence is reliable. This is necessary when approaching digital forensics from a scientific point of view and could be a legal requirement in the future. The core concept of this paper is the basic notion of abstraction layers. Abstraction layers exist in all forms of digital data and therefore in the tools used to analyze them. The idea of using tools for layers of abstraction is not new, but a discussion of the definitions, properties, and error types of abstraction layers when used with digital},
author = {Brian D. Carrier},
journal = {Int. J. Digit. EVid.},
volume = {1},
pages = {},
}

@article{9d80b3332699ba88e8fa73b9e231b2bdde2ab10a,
title = {In-Place File Carving},
year = {2007},
url = {https://www.semanticscholar.org/paper/9d80b3332699ba88e8fa73b9e231b2bdde2ab10a},
abstract = {File carving is the process of recovering files from an investigative target, potentially without knowledge of the filesystem structure. Current generation file carvers make complete copies of recovered files. Unfortunately, they often produce a large number of false positives — “junk” files with invalid formats that frequently consume large amounts of disk space.},
author = {G. Richard and Vassil Roussev and Lodovico Marziale},
doi = {10.1007/978-0-387-73742-3_15},
}

@article{a7ca96632060826e89dd053948431246532b895a,
title = {Predicting the types of file fragments},
year = {2008},
url = {https://www.semanticscholar.org/paper/a7ca96632060826e89dd053948431246532b895a},
abstract = {A problem that arises in computer forensics is to determine the type of a file fragment. An extension to the file name indicating the type is stored in the disk directory, but when a file is deleted, the entry for the file in the directory may be overwritten. This problem is easily solved when the fragment includes the initial header, which contains explicit type-identifying information, but it is more difficult to determine the type of a fragment from the middle of a file. We investigate two algorithms for predicting the type of a fragment: one based on Fisher's linear discriminant and the other based on longest common subsequences of the fragment with various sets of test files. We test the ability of the algorithms to predict a variety of common file types. Algorithms of this kind may be useful in designing the next generation of file-carvers - programs that reconstruct files when directory information is lost or deleted. These methods may also be useful in designing virus scanners, firewalls and search engines to find files that are similar to a given file.},
author = {William C. Calhoun and D. Coles},
journal = {Digit. Investig.},
volume = {5},
pages = {S14-S20},
doi = {10.1016/J.DIIN.2008.05.005},
}

@article{de2982cea68f8fcfea7fc94c697a6450a8e3aec9,
title = {Frame-Based Recovery of Corrupted Video Files Using Video Codec Specifications},
year = {2014},
url = {https://www.semanticscholar.org/paper/de2982cea68f8fcfea7fc94c697a6450a8e3aec9},
abstract = {In digital forensics, recovery of a damaged or altered video file plays a crucial role in searching for evidences to resolve a criminal case. This paper presents a frame-based recovery technique of a corrupted video file using the specifications of a codec used to encode the video data. A video frame is the minimum meaningful unit of video data. Many existing approaches attempt to recover a video file using file structure rather than frame structure. In case a target video file is severely fragmented or even has a portion of video overwritten by other video content, however, video file recovery of existing approaches may fail. The proposed approach addresses how to extract video frames from a portion of video to be restored as well as how to connect extracted video frames together according to the codec specifications. Experiment results show that the proposed technique successfully restores fragmented video files regardless of the amount of fragmentations. For a corrupted video file containing overwritten segments, the proposed technique can recover most of the video content in non-overwritten segments of the video file.},
author = {Gi-Hyun Na and Kyu-Sun Shim and K. Moon and S. Kong and Eun-Soo Kim and Joong Lee},
journal = {IEEE Transactions on Image Processing},
volume = {23},
pages = {517-526},
doi = {10.1109/TIP.2013.2285625},
pmid = {24235253},
}

@article{51eb53af678d56b02e01d5fee93ce08fd37a6507,
title = {File Fragment Classification-The Case for Specialized Approaches},
year = {2009},
url = {https://www.semanticscholar.org/paper/51eb53af678d56b02e01d5fee93ce08fd37a6507},
abstract = {Increasingly advances in file carving, memory analysis and network forensics requires the ability to identify the underlying type of a file given only a file fragment. Work to date on this problem has relied on identification of specific byte sequences in file headers and footers, and the use of statistical analysis and machine learning algorithms taken from the middle of the file. We argue that these approaches are fundamentally flawed because they fail to consider the inherent internal structure in widely used file types such as PDF, DOC, and ZIP. We support our argument with a bottom-up examination of some popular formats and an analysis of TK PDF files. Based on our analysis, we argue that specialized methods targeted to each specific file type will be necessary to make progress in this area.},
author = {Vassil Roussev and S. Garfinkel},
journal = {2009 Fourth International IEEE Workshop on Systematic Approaches to Digital Forensic Engineering},
volume = {},
pages = {3-14},
doi = {10.1109/SADFE.2009.21},
}

@article{d5baf4efe788cb54e5beb88283d01acb49d335ef,
title = {Computer Forensics: Incident Response Essentials},
year = {2001},
url = {https://www.semanticscholar.org/paper/d5baf4efe788cb54e5beb88283d01acb49d335ef},
abstract = {Preface. Acknowledgments. 1. Introduction to Computer Forensics. 2. Tracking an Offender. 3. The Basics of Hard Drives and Storage. 4. Encryption and Forensics. 5. Data Hiding. 6. Hostile Code. 7. Your Electronic Toolkit. 8. Investigating Windows Computers. 9. Introduction to Unix for Forensic Examiners. 10. Compromising a Unix Host. 11. Investigating a Unix Host. 12. Introduction to the Criminal Justice System. 13. Conclusion. Appendix A. Internet Data Center Response Plan. Appendix B. Incident Response Triage Questionnaire. Appendix C. How to Become a Unix Guru. Appendix D. Exporting a Windows 2000 Personal Certificate. Appendix E. How to Crowbar Unix Hosts. Appendix F. Creating a Linux Boot CD. Appendix G. Contents of a Forensic CD. Annotated Bibliography. Index. 0201707195T09182001},
author = {W. G. Kruse and Jay G. Heiser},
}

@article{0af5fb0021836866914492756377fb60b33b0676,
title = {Towards an Engineering Approach to File Carver Construction},
year = {2011},
url = {https://www.semanticscholar.org/paper/0af5fb0021836866914492756377fb60b33b0676},
abstract = {File carving is the process of recovering files without the help of (file system) storage metadata. A host of techniques exist to perform file carving, often used in several tools in varying combinations and implementations. This makes it difficult to determine what tool to use in specific investigations or when recovering files in a specific file format. We define recoverability as the set of software requirements for a file carver to recover files in a specified file format. This set can then be used to evaluate what tool to use or which technique to implement, based on external factors such as file format to recover, available time, engineering capacity and data set characteristics. File carving techniques are divided into two groups, format validation and file reconstruction. These groups refer to different parts of a file carver's implementation. Additionally, some techniques may be emphasized or omitted not only because of file format support for them, but based on performance effects that may result from applying them. We discuss a simplified variant of the GIF image file format as an example and show how a structured analysis of the format leads to design decisions for a file carver.},
author = {L. Aronson and J. V. D. Bos},
journal = {2011 IEEE 35th Annual Computer Software and Applications Conference Workshops},
volume = {},
pages = {368-373},
doi = {10.1109/COMPSACW.2011.68},
}

@article{c6679987a9b5e8672bec243caaf7b3f4ed7fca55,
title = {Carving contiguous and fragmented files with fast object validation},
year = {2007},
url = {https://www.semanticscholar.org/paper/c6679987a9b5e8672bec243caaf7b3f4ed7fca55},
abstract = {''File carving'' reconstructs files based on their content, rather than using metadata that points to the content. Carving is widely used for forensics and data recovery, but no file carvers can automatically reassemble fragmented files. We survey files from more than 300 hard drives acquired on the secondary market and show that the ability to reassemble fragmented files is an important requirement for forensic work. Next we analyze the file carving problem, arguing that rapid, accurate carving is best performed by a multi-tier decision problem that seeks to quickly validate or discard candidate byte strings - ''objects'' - from the media to be carved. Validators for the JPEG, Microsoft OLE (MSOLE) and ZIP file formats are discussed. Finally, we show how high speed validators can be used to reassemble fragmented files.},
author = {S. Garfinkel},
journal = {Digit. Investig.},
volume = {4},
pages = {2-12},
doi = {10.1016/J.DIIN.2007.06.017},
}

@article{f21e4ab2346ddfc8e9051a7218c1356647dc3bf1,
title = {Digital media triage with bulk data analysis and bulk_extractor},
year = {2013},
url = {https://www.semanticscholar.org/paper/f21e4ab2346ddfc8e9051a7218c1356647dc3bf1},
abstract = {Bulk data analysis eschews file extraction and analysis, common in forensic practice today, and instead processes data in "bulk," recognizing and extracting salient details ("features") of use in the typical digital forensics investigation. This article presents the requirements, design and implementation of the bulk_extractor, a high-performance carving and feature extraction tool that uses bulk data analysis to allow the triage and rapid exploitation of digital media. Bulk data analysis and the bulk_extractor are designed to complement traditional forensic approaches, not replace them. The approach and implementation offer several important advances over today's forensic tools, including optimistic decompression of compressed data, context-based stop-lists, and the use of a "forensic path" to document both the physical location and forensic transformations necessary to reconstruct extracted evidence. The bulk_extractor is a stream-based forensic tool, meaning that it scans the entire media from beginning to end without seeking the disk head, and is fully parallelized, allowing it to work at the maximum I/O capabilities of the underlying hardware (provided that the system has sufficient CPU resources). Although bulk_extractor was developed as a research prototype, it has proved useful in actual police investigations, two of which this article recounts.},
author = {S. Garfinkel},
journal = {Comput. Secur.},
volume = {32},
pages = {56-72},
doi = {10.1016/j.cose.2012.09.011},
}

@article{0645eb48828dd5e46df66ba4fa0df69ef9c341da,
title = {A study on multimedia file carving method},
year = {2012},
url = {https://www.semanticscholar.org/paper/0645eb48828dd5e46df66ba4fa0df69ef9c341da},
abstract = {File carving is a method that recovers files at unallocated space without any file information and used to recover data and execute a digital forensic investigation. In general, the file carving recovers files using the inherent header and footer in files or the entire file size determined in the file header. The largely used multimedia files, such as AVI, WAV, and MP3, can be exactly recovered using an internal format in files as they are continuously allocated. In the case of the NTFS, which is one of the most widely used file system, it supports an internal data compression function itself, but the NTFS compression function has not been considered in file carving. Thus, a large part of file carving tools cannot recover NTFS compressed files. Also, for carving the multimedia files compressed by the NTFS, a recovery method for such NTFS compressed files is required. In this study, we propose a carving method for multimedia files and represent a recovery plan for deleted NTFS compressed files. In addition, we propose a way to apply such a recovery method to the carving of multimedia files.},
author = {Byeongyeong Yoo and Jungheum Park and Sungsu Lim and Jewan Bang and Sangjin Lee},
journal = {Multimedia Tools and Applications},
volume = {61},
pages = {243-261},
doi = {10.1007/s11042-010-0704-y},
}

@article{c5b828456c07110aaa1d714bf9f16d9a1db871f1,
title = {The evolution of file carving},
year = {2009},
url = {https://www.semanticscholar.org/paper/c5b828456c07110aaa1d714bf9f16d9a1db871f1},
abstract = {Presents the evolution of file carving and describes in detail the techniques that are now being used to recover files without using any file system meta-data information. We show the benefits and problems that exist with current techniques. In the future, solid-state devices (SSDs) will become much more prevalent. SSDs will incorporate wear-leveling, which results in files being moved around so as to not allow some clusters to be written to more than others. This is done because after a certain amount of writes a cluster will fail and, therefore, the SSD controller will attempt to spread the write load across all clusters in the disk. As a result, SSDs will be naturally fragmented, and should the disk controller fail the clusters on the disk will require file carving techniques to recover. There is a lot of research yet to be done in this area for data recovery. Finally, while Pal et. al's techniques are useful for recovering text and images, new weighting techniques need to be created for video, audio, executable and other file formats, thus allowing the recovery to extend to those formats.},
author = {A. Pal and N. Memon},
journal = {IEEE Signal Processing Magazine},
volume = {26},
pages = {59-71},
doi = {10.1109/MSP.2008.931081},
}

@article{a26c48cff9aaed7adea0fd7176ff358670c8474c,
title = {Detecting file fragmentation point using sequential hypothesis testing},
year = {2008},
url = {https://www.semanticscholar.org/paper/a26c48cff9aaed7adea0fd7176ff358670c8474c},
abstract = {File carving is a technique whereby data files are extracted from a digital device without the assistance of file tables or other disk meta-data. One of the primary challenges in file carving can be found in attempting to recover files that are fragmented. In this paper, we show how detecting the point of fragmentation of a file can benefit fragmented file recovery. We then present a sequential hypothesis testing procedure to identify the fragmentation point of a file by sequentially comparing adjacent pairs of blocks from the starting block of a file until the fragmentation point is reached. By utilizing serial analysis we are able to minimize the errors in detecting the fragmentation points. The performance results obtained from the fragmented test-sets of DFRWS 2006 and 2007 show that the method can be effectively used in recovery of fragmented files.},
author = {A. Pal and H. Sencar and N. Memon},
journal = {Digit. Investig.},
volume = {5},
pages = {S2-S13},
doi = {10.1016/J.DIIN.2008.05.015},
}

@article{0903a3cd71e25a475b1dc36e98233b9b6544ed18,
title = {Advanced File Carving Approaches for Multimedia Files},
year = {2011},
url = {https://www.semanticscholar.org/paper/0903a3cd71e25a475b1dc36e98233b9b6544ed18},
abstract = {File carving is a recovery technique that recovers files based on information about their structure and content without matching file system information. As files can be recovered from their content and/or file structure this technique is indispensable during digital forensics investigations. So far many approaches for the recovery of digital images have been proposed. The main contribution of this paper is a discussion of existing and new approaches for the recovery of multimedia files. After a short discussion of relevant multimedia file formats we present an overview of the current state-of-the-art in file carving. In the main part we focus on the implementation of a file carver for fragmented multimedia files. Finally, we summarize our findings and give an outlook with regard to post-processing files that have been recovered successfully.},
author = {R. Poisel and S. Tjoa and Paul Tavolato},
journal = {J. Wirel. Mob. Networks Ubiquitous Comput. Dependable Appl.},
volume = {2},
pages = {42-58},
doi = {10.22667/JOWUA.2011.12.31.042},
}

@article{c66ed5c05e534272c542e50d0ec652010e74f347,
title = {Identification and recovery of JPEG files with missing fragments},
year = {2009},
url = {https://www.semanticscholar.org/paper/c66ed5c05e534272c542e50d0ec652010e74f347},
abstract = {Recovery of fragmented files proves to be a challenging task for encoded files like JPEG. In this paper, we consider techniques for addressing two issues related to fragmented JPEG file recovery. First issue concerns more efficient identification of the next fragment of a file undergoing recovery. Second issue concerns the recovery of file fragments which cannot be linked to an existing image header or for which there is no available image header. Current file recovery approaches are not well suited to deal with these practical issues. In addressing these problems, we utilize JPEG file format specifications. More specifically, we propose a technique based on bit sequence matching to identify fragments created by the same Huffman code tables. We also address the construction of a pseudo header needed for recovery of stand-alone file fragments. Some experimental results are provided to support our claims.},
author = {H. Sencar and N. Memon},
doi = {10.1016/J.DIIN.2009.06.007},
}

@article{e9f29938444894da0b22cac27e3219cb7ac82b8b,
title = {Bin-Carver: Automatic recovery of binary executable files},
year = {2012},
url = {https://www.semanticscholar.org/paper/e9f29938444894da0b22cac27e3219cb7ac82b8b},
abstract = {File carving is the process of reassembling files from disk fragments based on the file content in the absence of file system metadata. By leveraging both file header and footer pairs, traditional file carving mainly focuses on document and image files such as PDF and JPEG. With the vast amount of malware code appearing in the wild daily, recovery of binary executable files becomes an important problem, especially for the case in which malware deletes itself after compromising a computer. However, unlike image files that usually have both a header and footer pair, executable files only have header information, which makes the carving much harder. In this paper, we present Bin-Carver, a first-of-its- kind system to automatically recover executable files with deleted or corrupted metadata. The key idea is to explore the road map information defined in executable file headers and the explicit control flow paths present in the binary code. Our experiment with thousands of binary code files has shown our Bin-Carver to be incredibly accurate, with an identification rate of 96.3% and recovery rate of 93.1% on average when handling file systems ranging from pristine to chaotic and highly fragmented.},
author = {S. Hand and Zhiqiang Lin and G. Gu and B. Thuraisingham},
journal = {Digit. Investig.},
volume = {9},
pages = {S108-S117},
doi = {10.1016/J.DIIN.2012.05.014},
}

@article{73f641206a7d4ba231e1ff1cd94eb1dec33d5420,
title = {Automated reassembly of fragmented images},
year = {2003},
url = {https://semanticscholar.org/paper/73f641206a7d4ba231e1ff1cd94eb1dec33d5420},
abstract = {In this paper we address the problem of reassembly of images from a collection of their fragments. The image reassembly problem is formulated as a combinatorial optimization problem and image assembly is then done by finding an optimal ordering of fragments. We present implementation results showing that images can be reconstructed with high accuracy even when there are thousands of fragments and multiple images involved.},
author = {Anandabrata  Pal and Kulesh  Shanmugasundaram and Nasir D. Memon},
journal = {2003 International Conference on Multimedia and Expo. ICME '03. Proceedings (Cat. No.03TH8698)},
volume = {1},
pages = {I-625},
doi = {10.1109/ICME.2003.1220995},
}

@article{38d903cab6b666043254aae5376ef4c79e79b128,
title = {JPGcarve: An Advanced Tool for Automated Recovery of Fragmented JPEG Files},
year = {2016},
url = {https://www.semanticscholar.org/paper/38d903cab6b666043254aae5376ef4c79e79b128},
abstract = {In this paper, we present a new tool for forensic recovery of single and multi-fragment JPEG/JFIF data files. First, we discuss the basic design and the technical methods composing our proposed data carving algorithm. Next, we compare the performance of our method with the well-known Adroit Photo Forensics (APF) state-of-the art tool. This comparison is centered on both the carving results as well as the obtained data processing speed, and is evaluated in terms of the results that can be obtained for several well-known reference data sets. Important to note is that we specifically focus on the fundamental recovery and fragment matching performance of the tools by forcing them to use various assumed cluster sizes. We show that on all accounts our new tool can significantly outperform APF. This improvement in data processing speed and carving results can be mostly attributed to novel methods to iterate and reduce the data search space and to a novel parameterless method to determine the end of a fragment based on the pixel data. Finally, we discuss several options for future research.},
author = {Johan de Bock and P. Smet},
journal = {IEEE Transactions on Information Forensics and Security},
volume = {11},
pages = {19-34},
doi = {10.1109/TIFS.2015.2475238},
}

@article{746eb9ef17ec9a127320a9e6b630569026b41199,
title = {Reassembly of Fragmented JPEG Images Containing Restart Markers},
year = {2008},
url = {https://www.semanticscholar.org/paper/746eb9ef17ec9a127320a9e6b630569026b41199},
abstract = {A fragmented JPEG image is currently not possible to reassemble without knowing the ordering of the fragments. This is a problem for the police when they search for illegal digital images. This paper presents a method to reassemble fragmented JPEG images containing restart markers. Empirical evaluations show that it is possible to reassemble images taken from a set containing fragments of several images.},
author = {M. Karresand and N. Shahmehri},
journal = {2008 European Conference on Computer Network Defense},
volume = {},
pages = {25-32},
doi = {10.1109/EC2ND.2008.10},
}

@article{c81d7162ef4dcd853033fcdf516b156ce08fcbf5,
title = {Scalpel: A Frugal, High Performance File Carver},
year = {2005},
url = {https://www.semanticscholar.org/paper/c81d7162ef4dcd853033fcdf516b156ce08fcbf5},
abstract = {File carving is an important technique for digital forensics investigation and for simple data recovery. By using a database of headers and footers (essentially, strings of bytes at predictable offsets) for specific file types, file carvers can retrieve files from raw disk images, regardless of the type of filesystem on the disk image. Perhaps more importantly, file carving is possible even if the filesystem metadata has been destroyed. This paper presents some requirements for high performance file carving, derived during design and implementation of Scalpel, a new open source file carving application. Scalpel runs on machines with only modest resources and performs carving operations very rapidly, outperforming most, perhaps all, of the current generation of carving tools. The results of a number of experiments are presented to support this assertion.},
author = {G. Richard and Vassil Roussev},
}

@article{4dc3503a1d74822ea6cf10c3590b61e257897c5c,
title = {Digital Forensic Research: The Good, the Bad and the Unaddressed},
year = {2009},
url = {https://www.semanticscholar.org/paper/4dc3503a1d74822ea6cf10c3590b61e257897c5c},
abstract = {Digital forensics is a relatively new scientific discipline, but one that has matured greatly over the past decade. In any field of human endeavor, it is important to periodically pause and review the state of the discipline. This paper examines where the discipline of digital forensics is at this point in time and what has been accomplished in order to critically analyze what has been done well and what ought to be done better. The paper also takes stock of what is known, what is not known and what needs to be known. It is a compilation of the author’s opinion and the viewpoints of twenty-one other practitioners and researchers, many of whom are leaders in the field. In synthesizing these professional opinions, several consensus views emerge that provide valuable insights into the “state of the discipline.”},
author = {Nicole Beebe},
doi = {10.1007/978-3-642-04155-6_2},
}

@article{762151ba1534960fcc11e3548ff8361ee2917382,
title = {File Type Identification of Data Fragments by Their Binary Structure},
year = {2006},
url = {https://www.semanticscholar.org/paper/762151ba1534960fcc11e3548ff8361ee2917382},
abstract = {Rapidly gaining information superiority is vital when fighting an enemy, but current computer forensics tools, which require file headers or a working file system to function, do not enable us to quickly map out the contents of corrupted hard disks or other fragmented storage media found at crime scenes. The lack of proper tools slows down the hunt for information, which would otherwise help in gaining the upper hand against IT based perpetrators. To address this problem, this paper presents an algorithm which allows categorization of data fragments based solely on their structure, without the need for any meta data. The algorithm is based on measuring the rate of change of the byte contents of digital media and extends the byte frequency distribution based Oscar method presented in an earlier paper. The evaluation of the new method shows a detection rate of 99.2 %, without generating any false positives, when used to scan for JPEG data. The slowest implementation of the algorithm scans a 72.2 MB file in approximately 2.5 seconds and scales linearly},
author = {M. Karresand and N. Shahmehri},
journal = {2006 IEEE Information Assurance Workshop},
volume = {},
pages = {140-147},
doi = {10.1109/IAW.2006.1652088},
}

@article{c50d682c000988d9b0af10fb41f59cf738c8e5a2,
title = {Automated reassembly of file fragmented images using greedy algorithms},
year = {2006},
url = {https://www.semanticscholar.org/paper/c50d682c000988d9b0af10fb41f59cf738c8e5a2},
abstract = {The problem of restoring deleted files from a scattered set of fragments arises often in digital forensics. File fragmentation is a regular occurrence in hard disks, memory cards, and other storage media. As a result, a forensic analyst examining a disk may encounter many fragments of deleted digital files, but is unable to determine the proper sequence of fragments to rebuild the files. In this paper, we investigate the specific case where digital images are heavily fragmented and there is no file table information by which a forensic analyst can ascertain the correct fragment order to reconstruct each image. The image reassembly problem is formulated as a k-vertex disjoint graph problem and reassembly is then done by finding an optimal ordering of fragments. We provide techniques for comparing fragments and describe several algorithms for image reconstruction based on greedy heuristics. Finally, we provide experimental results showing that images can be reconstructed with high accuracy even when there are thousands of fragments and multiple images involved.},
author = {N. Memon and Anindrabatha Pal},
journal = {IEEE Transactions on Image Processing},
volume = {15},
pages = {385-393},
doi = {10.1109/TIP.2005.863054},
pmid = {16479808},
}

@article{0a33ced4e895f9ea53bae8f358a522966f3fb601,
title = {Advanced carving techniques},
year = {2007},
url = {https://www.semanticscholar.org/paper/0a33ced4e895f9ea53bae8f358a522966f3fb601},
abstract = {Carving is the term most often used to indicate the act of recovering a file from unstructured digital forensic images. The term unstructured indicates that the original digital image does not contain useful filesystem information which may be used to assist in this recovery. Typically, forensic analysts resort to carving techniques as an avenue of last resort due to the difficulty of current techniques. Most current techniques rely on manual inspection of the file to be recovered and manually reconstructing this file using trial and error. Manual processing is typically impractical for modern disk images which might contain hundreds of thousands of files. At the same time the traditional process of recovering deleted files using filesystem information is becoming less practical because most modern filesystems purge critical information for deleted files. As such the need for automated carving techniques is quickly arising even when a filesystem does exist on the forensic image. This paper explores the theory of carving in a formal way. We then proceed to apply this formal analysis to the carving of PDF and ZIP files based on the internal structure inherent within the file formats themselves. Specifically this paper deals with carving from the Digital Forensic Research Work-Shop's (DFRWS) 2007 carving challenge.},
author = {Michael I. Cohen},
journal = {Digit. Investig.},
volume = {4},
pages = {119-128},
doi = {10.1016/j.diin.2007.10.001},
}

@article{a5dc9c53a26e0f7262f0809d1650a387ae648b77,
title = {Digital forensics research: The next 10 years},
year = {2010},
url = {https://www.semanticscholar.org/paper/a5dc9c53a26e0f7262f0809d1650a387ae648b77},
abstract = {Today's Golden Age of computer forensics is quickly coming to an end. Without a clear strategy for enabling research efforts that build upon one another, forensic research will fall behind the market, tools will become increasingly obsolete, and law enforcement, military and other users of computer forensics products will be unable to rely on the results of forensic analysis. This article summarizes current forensic research directions and argues that to move forward the community needs to adopt standardized, modular approaches for data representation and forensic processing.},
author = {S. Garfinkel},
journal = {Digital Investigation},
volume = {7},
pages = {},
doi = {10.1016/J.DIIN.2010.05.009},
}

@article{660bc36278483acf080f4cc34b56fbf5f80b36e6,
title = {Oscar - File Type Identification of Binary Data in Disk Clusters and RAM Pages},
year = {2006},
url = {https://www.semanticscholar.org/paper/660bc36278483acf080f4cc34b56fbf5f80b36e6},
abstract = {This paper proposes a method, called Oscar, for determining the probable file type of binary data fragments. The Oscar method is based on building models, called centroids, of the mean and standard deviation of the byte frequency distribution of different file types. A weighted quadratic distance metric is then used to measure the distance between the centroid and sample data fragments. If the distance falls below a threshold, the sample is categorized as probably belonging to the modelled file type. Oscar is tested using JPEG pictures and is shown to give a high categorization accuracy, i.e. high detection rate and low false positives rate. By using a practical example we demonstrate how to use the Oscar method to prove the existence of known pictures based on fragments of them found in RAM and the swap partition of a computer.},
author = {M. Karresand and N. Shahmehri},
doi = {10.1007/0-387-33406-8_35},
}

@article{a4d0eb038b3e9ecc74a5bf64573e379f7b0193c3,
title = {Advanced JPEG carving},
year = {2008},
url = {https://www.semanticscholar.org/paper/a4d0eb038b3e9ecc74a5bf64573e379f7b0193c3},
abstract = {Data carving is a digital forensic technique which aims to reconstitute a file from unstructured data sources with no knowledge of the previously stored file system. This paper presents an approach for the carving of JPEG files. Since JPEG is one of the most popular image formats in the storage and distribution of digital photographic imagery it is frequently of great interest for certain types of forensic investigations.
 We apply the previously developed carving theory to the JPEG image format and develop an accurate, fully automated carver.
 We present a novel technique for the suspension and resumption of the decoder, making it possible to carve JPEG images in an acceptable time.},
author = {Michael I. Cohen},
doi = {10.4108/E-FORENSICS.2008.2643},
}

@article{3e5c75416653a052ef305de677b4ed0d4f5f605d,
title = {The Normalised Compression Distance as a file fragment classifier},
year = {2010},
url = {https://www.semanticscholar.org/paper/3e5c75416653a052ef305de677b4ed0d4f5f605d},
abstract = {We have applied the generalised and universal distance measure NCD-Normalised Compression Distance-to the problem of determining the type of file fragments. To enable later comparison of the results, the algorithm was applied to fragments of a publicly available corpus of files. The NCD algorithm in conjunction with the k-nearest-neighbour (k ranging from one to ten) as the classification algorithm was applied to a random selection of circa 3000 512-byte file fragments from 28 different file types. This procedure was then repeated ten times. While the overall accuracy of the n-valued classification only improved the prior probability from approximately 3.5% to circa 32-36%, the classifier reached accuracies of circa 70% for the most successful file types. A prototype of a file fragment classifier was then developed and evaluated on new set of data (from the same corpus). Some circa 3000 fragments were selected at random and the experiment repeated five times. This prototype classifier remained successful at classifying individual file types with accuracies ranging from only slightly lower than 70% for the best class, down to similar accuracies as in the prior experiment.},
author = {Stefan Axelsson},
journal = {Digit. Investig.},
volume = {7},
pages = {S24-S31},
doi = {10.1016/J.DIIN.2010.05.004},
}

@article{69f764064d2fc6d369451234bc662d2f20214bcd,
title = {Statistical Disk Cluster Classification for File Carving},
year = {2007},
url = {https://semanticscholar.org/paper/69f764064d2fc6d369451234bc662d2f20214bcd},
abstract = {File carving is the process of recovering files from a disk without the help of a file system. In forensics, it is a helpful tool in finding hidden or recently removed disk content. Known signatures in file headers and footers are especially useful in carving such files out, that is, from header until footer. However, this approach assumes that file clusters remain in order. In case of file fragmentation, file clusters can be disconnected and the order can even be disrupted such that straighforward carving will fail. In this paper, we focus on methods for classifying clusters into file types by using the statistics of the clusters. By not exploiting the possible embedded signatures, we generate evidence from a different source that can be integrated later on. We propose a set of characteristic features and use statistical pattern recognition to learn a supervised classification model for a range of relevant file types. We exploit the statistics of a restricted number of neighboring clusters (context) to improve classification performance. In the experiments we show that the proposed features indeed enable the differentation of clusters into file types. Moreover, for some file types the incorporation of cluster context improves the recognition performance significantly.},
author = {Cor J. Veenman},
journal = {Third International Symposium on Information Assurance and Security},
volume = {},
pages = {393-398},
doi = {10.1109/IAS.2007.79},
}

@article{9b8f2406152c6128f246b40f7aa6439367735dbb,
title = {SÁDI - Statistical Analysis for Data Type Identification},
year = {2008},
url = {https://www.semanticscholar.org/paper/9b8f2406152c6128f246b40f7aa6439367735dbb},
abstract = {A key task in digital forensic analysis is the location of relevant information within the computer system. Identification of the relevancy of data is often dependent upon the identification of the type of data being examined. Typical file type identification is based upon file extension or magic keys. These typical techniques fail in many typical forensic analysis scenarios such as needing to deal with embedded data, such as with Microsoft Word files, or file fragments. The SADI (Statistical Analysis Data Identification) technique applies statistical analysis of the byte values of the data in such a way that the accuracy of the technique does not rely on the potentially misleading metadata information but rather the values of the data itself. The development of SADI provides the capability to identify what digitally stored data actually represents and will also allow for the selective extraction of portions of the data for additional investigation; i.e., in the case of embedded data. Thus, our research provides a more effective type identification technique that does not fail on file fragments, embedded data types, or with obfuscated data.},
author = {Sarah J. Moody and R. Erbacher},
journal = {2008 Third International Workshop on Systematic Approaches to Digital Forensic Engineering},
volume = {},
pages = {41-54},
doi = {10.1109/SADFE.2008.13},
}

@article{23a4a8ccd9677a69b7a525370f02dd62640f5f6c,
title = {File Block Classification by Support Vector Machine},
year = {2011},
url = {https://www.semanticscholar.org/paper/23a4a8ccd9677a69b7a525370f02dd62640f5f6c},
abstract = {Retrieval of files without the support of file system structures is arguably essential for digital forensics. Files are typically stored as sequences of data blocks, which have to be reconstructed in the retrieval process. This is commonly performed, among other approaches, through file carving, in general detecting the original block sequences by means of signatures of known headers and footers of files. Of course, this creates challenges with fragmented files, where blocks belonging to different files may be interleaved. Ways to classify file blocks into file types relying on their content may provide a support to achieve a successful reconstruction. We propose to classify file blocks using Support Vector Machines (SVMs), and we do so by studying in-depth the impact of an appropriate selection of the features used in the classification process. We analyze several potential features and test their performance over a large and representative collection of file blocks and file types. We find out that SVM classifiers can achieve a good accuracy and that a specific type of features (based on byte frequency distribution) performs well across almost all of the examined file types.},
author = {L. Sportiello and S. Zanero},
journal = {2011 Sixth International Conference on Availability, Reliability and Security},
volume = {},
pages = {307-312},
doi = {10.1109/ARES.2011.52},
}

@article{324013a7023a159975a2c68839f2f9900e89f88b,
title = {Roadmap to Approaches for Carving of Fragmented Multimedia Files},
year = {2011},
url = {https://www.semanticscholar.org/paper/324013a7023a159975a2c68839f2f9900e89f88b},
abstract = {File carving is a recovery technique which does not consider file tables or other meta-data which is used to organize data on storage media. As files can be recovered based only on their content and/or structure this technique is an indispensable task during digital investigations. The main contribution of this paper is a survey about new approaches in the file carving research field and a roadmap that outlines the necessary steps towards video file carving. So far many approaches for the recovery of digital images have been proposed. After a short discussion of relevant representatives in this domain we focus on the applicability of these approaches to the recovery of multimedia files. Further this paper discusses ideas from the forensics wiki for their applicability to such a file carver. Finally our findings are summarized verbally and visually as a roadmap.},
author = {R. Poisel and S. Tjoa},
journal = {2011 Sixth International Conference on Availability, Reliability and Security},
volume = {},
pages = {752-757},
doi = {10.1109/ARES.2011.118},
}

@article{0cc2a1842b3977dbdec3fa32c33fabf6f50e98ff,
title = {Forensic Discovery},
year = {2004},
url = {https://www.semanticscholar.org/paper/0cc2a1842b3977dbdec3fa32c33fabf6f50e98ff},
abstract = {"Don't look now, but your fingerprints are all over the cover of this book. Simply picking it up off the shelf to read the cover has left a trail of evidence that you were here."If you think book covers are bad, computers are worse. Every time you use a computer, you leave elephant-sized tracks all over it. As Dan and Wietse show, even people trying to be sneaky leave evidence all over, sometimes in surprising places."This book is about computer archeology. It's about finding out what might have been based on what is left behind. So pick up a tool and dig in. There's plenty to learn from these masters of computer security."--Gary McGraw, Ph.D., CTO, Cigital, coauthor of Exploiting Software and Building Secure Software "A wonderful book. Beyond its obvious uses, it also teaches a great deal about operating system internals."--Steve Bellovin, coauthor of Firewalls and Internet Security, Second Edition, and Columbia University professor "A must-have reference book for anyone doing computer forensics. Dan and Wietse have done an excellent job of taking the guesswork out of a difficult topic."--Brad Powell, chief security architect, Sun Microsystems, Inc. "Farmer and Venema provide the essential guide to 'fossil' data. Not only do they clearly describe what you can find during a forensic investigation, they also provide research found nowhere else about how long data remains on disk and in memory. If you ever expect to look at an exploited system, I highly recommend reading this book."--Rik Farrow, Consultant, author of Internet Security for Home and Office "Farmer and Venema do for digital archaeology what Indiana Jones did for historical archaeology. Forensic Discovery unearths hidden treasures in enlightening and entertaining ways, showing how a time-centric approach to computer forensics reveals even the cleverest intruder."--Richard Bejtlich, technical director, ManTech CFIA, and author of The Tao of Network Security Monitoring "Farmer and Venema are 'hackers' of the old school: They delight in understanding computers at every level and finding new ways to apply existing information and tools to the solution of complex problems."--Muffy Barkocy, Senior Web Developer, Shopping.com "This book presents digital forensics from a unique perspective because it examines the systems that create digital evidence in addition to the techniques used to find it. I would recommend this book to anyone interested in learning more about digital evidence from UNIX systems."--Brian Carrier, digital forensics researcher, and author of File System Forensic AnalysisThe Definitive Guide to Computer Forensics: Theory and Hands-On Practice Computer forensics--the art and science of gathering and analyzing digital evidence, reconstructing data and attacks, and tracking perpetrators--is becoming ever more important as IT and law enforcement professionals face an epidemic in computer crime. In Forensic Discovery, two internationally recognized experts present a thorough and realistic guide to the subject. Dan Farmer and Wietse Venema cover both theory and hands-on practice, introducing a powerful approach that can often recover evidence considered lost forever. The authors draw on their extensive firsthand experience to cover everything from file systems, to memory and kernel hacks, to malware. They expose a wide variety of computer forensics myths that often stand in the way of success. Readers will find extensive examples from Solaris, FreeBSD, Linux, and Microsoft Windows, as well as practical guidance for writing one's own forensic tools. The authors are singularly well-qualified to write this book: They personally created some of the most popular security tools ever written, from the legendary SATAN network scanner to the powerful Coroner's Toolkit for analyzing UNIX break-ins. After reading this book you will be able to Understand essential forensics concepts: volatility, layering, and trust Gather the maximum amount of reliable evidence from a running system Recover partially destroyed information--and make sense of it Timeline your system: understand what really happened when Uncover secret changes to everything from system utilities to kernel modules Avoid cover-ups and evidence traps set by intruders Identify the digital footprints associated with suspicious activity Understand file systems from a forensic analyst's point of view Analyze malware--without giving it a chance to escape Capture and examine the contents of main memory on running systems Walk through the unraveling of an intrusion, one step at a time The book's companion Web site contains complete source and binary code for open source software discussed in the book, plus additional computer forensics case studies and resource links.},
author = {D. Farmer and W. Venema},
}

@article{fdedc24a6957e7dd4a824e72160a26aa5a18209f,
title = {Automatic Reassembly of Document Fragments via Data Compression},
year = {2002},
url = {https://www.semanticscholar.org/paper/fdedc24a6957e7dd4a824e72160a26aa5a18209f},
abstract = {Reassembly of fragmented objects from a collection of randomly mixed fragments is a common problem in classical forensics. In this paper we address the digital forensic equivalent, i.e., reassembly of document fragments, using statistical modelling tools applied in data compression. We propose a general process model for automatically analyzing a collection fragments to reconstruct the original document by placing the fragments in proper order. Probabilities are assigned to the likelihood that two given fragments are adjacent in the original using context modelling techniques in data compression. The problem of finding the optimal ordering is shown to be equivalent to finding a maximum weight Hamiltonian path in a complete graph. Heuristics are designed and explored and implementation results provided which demonstrate the validity of the proposed technique.},
author = {K. Shanmugasundaram and N. Memon},
}

@article{e6bc8e5305d90fb2bbd28cd594a10ea393f3d962,
title = {Carving Orphaned JPEG File Fragments},
year = {2015},
url = {https://www.semanticscholar.org/paper/e6bc8e5305d90fb2bbd28cd594a10ea393f3d962},
abstract = {File carving techniques allow for recovery of files from storage devices in the absence of any file system metadata. When data are encoded and compressed, the current paradigm of carving requires the knowledge of the compression and encoding settings to succeed. In this paper, we advance the state of the art in JPEG file carving by introducing the ability to recover fragments of a JPEG file when the associated file header is missing. To realize this, we examined JPEG file headers of a large number of images collected from Flickr photo sharing site to identify their structural characteristics. Our carving approach utilizes this information in a new technique that performs two tasks. First, it decompresses the incomplete file data to obtain a spatial domain representation. Second, it determines the spatial domain parameters to produce a perceptually meaningful image. Recovery results on a variety of JPEG file fragments show that given the knowledge of Huffman code tables, our technique can very reliably identify the remaining decoder settings for all fragments of size 4 KiB or above. Although errors due to detection of image width, placement of image blocks, and color and brightness adjustments can occur, these errors reduce significantly when fragment sizes are >32 KiB.},
author = {Erkam Uzun and H. Sencar},
journal = {IEEE Transactions on Information Forensics and Security},
volume = {10},
pages = {1549-1563},
doi = {10.1109/TIFS.2015.2416685},
}

@article{ee68d4b6eaa4d07bb7f64410b511bf5bb2dfe182,
title = {Fileprints: identifying file types by n-gram analysis},
year = {2005},
url = {https://www.semanticscholar.org/paper/ee68d4b6eaa4d07bb7f64410b511bf5bb2dfe182},
abstract = {We propose a method to analyze files to categorize their type using efficient 1-gram analysis of their binary contents. Our aim is to be able to accurately identify the true type of an arbitrary file using statistical analysis of their binary contents without parsing. Consequently, we may determine the type of a file if its name does not announce its true type. The method represents each file type by a compact representation we call a fileprint, effectively a simple means of representing all members of the same file type by a set of statistical 1-gram models. The method is designed to be highly efficient so that files can be inspected with little or no buffering, and on a network appliance operating in high bandwidth environment or when streaming the file from or to disk.},
author = {Wei-Jen Li and Ke Wang and S. Stolfo and B. Herzog},
journal = {Proceedings from the Sixth Annual IEEE SMC Information Assurance Workshop},
volume = {},
pages = {64-71},
doi = {10.1109/IAW.2005.1495935},
}
